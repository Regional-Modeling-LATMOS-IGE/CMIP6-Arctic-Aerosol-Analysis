{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc97a7c-a97e-4464-9d20-e22becb5685b",
   "metadata": {},
   "source": [
    "# Downloading an ensemble of CMIP6 data with a series of criterias thanks to intake-esgf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b1ede-2de5-487c-9602-6b1b339c6e44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Purpose of the notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44b794",
   "metadata": {},
   "source": [
    "This notebook aims at **downloading an ensemble of CMIP6 variables from python** thanks to a dictionary of **user-defined criterias**.  \n",
    "\n",
    "All the links of the documents were accessed on the **25/03/2025**.\n",
    "\n",
    "It uses the *intake-esgf* library : https://github.com/esgf2-us/intake-esgf?tab=readme-ov-file. A beginner guide for this library can be found there : https://intake-esgf.readthedocs.io/en/latest/beginner.html.\n",
    "\n",
    "A detailed documentation for the CMIP6 can be found here : https://wcrp-cmip.org/cmip-model-and-experiment-documentation/.\n",
    "\n",
    "\n",
    "\n",
    "Feel free to share, use and improve the following code according to the provided license on the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dc94a-da9d-4935-b2f2-980f174b2e94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model outputs searched\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3801d",
   "metadata": {},
   "source": [
    "Every search needs to be constrained by the attributes of the model outputs we are looking for. A detailed document listing these attributes can be found here : https://docs.google.com/document/d/1h0r8RZr_f3-8egBMMh7aqLwy3snpD6_MrDz1q8n5XUk/edit?tab=t.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fed764-1686-4617-b8db-4eb9124a0030",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7090b8-596e-4ed9-bab7-6ace8abf1da2",
   "metadata": {},
   "source": [
    "We use two experiments realized during the CMIP6  : **piClim-control** and **piClim-aer**. These are both atmosphere-only climate model simulations in which sea surface temperatures (SSTs) and sea icea concentrations (SICs) are fixed at model-specific preindustrial climatological values. The description of the experiments can be found here : https://wcrp-cmip.github.io/CMIP6_CVs/docs/CMIP6_experiment_id.html.\n",
    "\n",
    "> **piClim-control** : assumes aerosols' burdens set to their preindustrial levels, it is the control experiment.\n",
    "> \n",
    "> **piClim-aer** : uses present-day, present-day being 2014, aerosols burdens' levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9468df7-ef52-4b1f-a170-d8aaef381f57",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780d594-a1a1-4332-8a62-bb9aae82697f",
   "metadata": {},
   "source": [
    "The variable used are listed and explicited below according to : https://clipc-services.ceda.ac.uk/dreq/mipVars.html.\n",
    "\n",
    "> <span style=\"color:SkyBlue\">**clt**</span>  : Total cloud area fraction (%) for the whole atmospheric column\n",
    ">\n",
    "> <span style=\"color:gold\">**rsdt**</span> : Shortwave radiation ($W/m^{2}$) **incident** at the TOA\n",
    "> \n",
    "> <span style=\"color:orange\">**rsut**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at the TOA\n",
    ">\n",
    "> <span style=\"color:orangered\">**rsutcs**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at TOA for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:Orchid\">**rsds**</span> : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface\n",
    "> \n",
    "> <span style=\"color:Indigo \">**rsdscs**</span>  : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:YellowGreen\">**rsus**</span> : Shortwave **upwelling** radiation ($W/m^{2}$) at the surface\n",
    ">\n",
    "> <span style=\"color:Darkgreen\">**rsuscs**</span>: Shortwave **upwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    ">\n",
    "> **areacella** : For every grid, the latitude-dependent surface associated to each grid point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389886f6-2e8d-4d99-b4cc-71ebd86e01fb",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a82e4-19a5-4bb9-aa5d-ccb2c6260107",
   "metadata": {},
   "source": [
    "The table sets how the variables are organized. We use the **AERmon** table. The details about the tables can be found here : https://clipc-services.ceda.ac.uk/dreq/index/miptable.html.\n",
    "\n",
    "> **Amon** stands for a set of monthly atmospheric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb1cc5-febe-4ebf-8dca-b8700098e548",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdebe37-a1de-4df2-a44b-1115d2a8c4dd",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63355f-8ed1-4a49-aef4-82919ba07cf8",
   "metadata": {},
   "source": [
    "We import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dab12c-5141-4e6f-a908-aa5792b296ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ IMPORTATIONS ================ #\n",
    "\n",
    "import intake_esgf  # this gives us access to the ESGF catalog to make queries\n",
    "\n",
    "import pandas as pd  # to manage the product of the search\n",
    "\n",
    "import numpy as np  # to manage the pandas arrays\n",
    "\n",
    "import os  # to get access to commands related to path setting and creation of directories\n",
    "\n",
    "from utilities.download.folders_handle.create import (\n",
    "    create_dir,\n",
    ")  # function to create a cleaned downloading directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474b9b5-edeb-4cc5-829c-035af5c986ff",
   "metadata": {},
   "source": [
    "### Set our search criterias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d13d48-a18d-4f4c-ae0c-32719909fdc6",
   "metadata": {},
   "source": [
    "Here the user may define its search criterias. We create a dictionary structure that we update with determined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8159458-7777-4fe5-83af-84d953ea377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ SEARCH CRITERIAS FOR OUR ANALYSIS ================ #\n",
    "\n",
    "### EXPERIMENTS ###\n",
    "\n",
    "experiment_id = [\n",
    "    \"piClim-control\",\n",
    "    \"piClim-aer\",\n",
    "]\n",
    "\n",
    "### VARIABLES ###\n",
    "\n",
    "variable_id = [\n",
    "    \"clt\",\n",
    "    \"rsdt\",\n",
    "    \"rsut\",\n",
    "    \"rsutcs\",\n",
    "    \"rsds\",\n",
    "    \"rsus\",\n",
    "    \"rsdscs\",\n",
    "    \"rsuscs\",\n",
    "]\n",
    "\n",
    "### TABLE ###\n",
    "\n",
    "table_id = \"Amon\"\n",
    "\n",
    "### DEFINE SEARCH CRITERIAS dictionary ###\n",
    "\n",
    "search = {\n",
    "    \"experiment_id\" : experiment_id,\n",
    "    \"variable_id\" : variable_id,\n",
    "    \"table_id\" : table_id\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9f4bd-93d4-48be-8384-917bc9a707c2",
   "metadata": {},
   "source": [
    "### Create the folder in which the data will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11ada8",
   "metadata": {},
   "source": [
    "Here the user can chose to create the folder to store the data. It will erase a pre-existing folder if the *do_we_clear* option is set to **True**. This can be quite slow if the data folder is already holding some heavy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CHOSE IF WE CLEAR AN ALREADY EXISTING FOLDER ================ #\n",
    "\n",
    "do_we_clear = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415429ca-8b8a-4fbb-9aa5-75775894c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CREATE THE FOLDER TO STORE THE DOWNLOADED DATA ================ #\n",
    "\n",
    "### DEFINE WHERE TO MOVE THE FILES AT THE END ###\n",
    "\n",
    "## Home directory ##\n",
    "\n",
    "homedir_path = os.path.expanduser(\"~\")\n",
    "\n",
    "## Parent directory ##\n",
    "\n",
    "parent_path = homedir_path + \"/certainty-data\"\n",
    "\n",
    "## Name of the created folder ##\n",
    "\n",
    "downloading_folder_name = \"CMIP6-DATA\"\n",
    "\n",
    "### CREATE THE DIRECTORY AND EMPTY IF MAKE_A_NEW_FOLDER ###\n",
    "\n",
    "downloading_path = create_dir(\n",
    "    parent_path=parent_path, name=downloading_folder_name, clear=do_we_clear\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The downloading folder {} is under the path {}.\".format(\n",
    "        downloading_folder_name, downloading_path\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.download.load_cmip6 import loading_cmip6\n",
    "\n",
    "full_cmip6_dict, areacella_dict = loading_cmip6(\n",
    "    parent_path = parent_path, \n",
    "    downloading_folder_name = downloading_folder_name, \n",
    "    case = \"ZELINKA-SW\", \n",
    "    do_we_clear = True,\n",
    "    remove_ensembles = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c8be4-7c3d-4206-8519-af1898501d0c",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure the ESGFCatalog\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856c1ec-613e-414c-9690-086a43e26a16",
   "metadata": {},
   "source": [
    "The ESGFCatalog is initially parametrized with default values on may want to change. We will focus on three main changes :\n",
    "\n",
    "* **defining the nodes that the query will investigate** \n",
    "\n",
    "* **setting where the data will be downloaded** \n",
    "\n",
    "* **adding a path that is specific to our cluster to search for CMIP6 outputs locally** \n",
    "\n",
    "The default configuration of the catalog can be accessed through the following line of code. More details on the configuration may be found here : https://intake-esgf.readthedocs.io/en/latest/configure.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4a1db-5472-4a50-87d1-eb44f75d0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intake_esgf.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37676875-e50f-443d-98ef-50ef80c75b5a",
   "metadata": {},
   "source": [
    "There are some variables that are of interest to us :\n",
    "\n",
    "* The *solr_indices* and the *globus_indices* variables define on which nodes the query is realized. \n",
    "\n",
    "* The *local_cache* variable sets where the data will be downloaded.\n",
    "\n",
    "* The *esg_dataroot* variable sets local path to explore before downloading any data.\n",
    "\n",
    "We can see that, as said in the documentation of the intake_esgf library, the search is done by default with Globus-based indices at the holdings of OLCF (Oak Ridge Leadership Computing Facility) and ALCF (Argonne Leadership Computing Facility). We may extend the search to all the possible ESGF nodes in order to not miss any model output. Note that the solr nodes are way much slower than the globus ones.\n",
    "\n",
    "By default, the folder in which is stored the downloaded data is ~/.esgf/. This is a hidden folder in your home repository. It is more convenient, if you are working on a shared resource such as an institutional cluster or group workstation, to define it on a directory dedicated to data.\n",
    "\n",
    "Finally, if we are working on a cluster having some access to CMIP6 data, it is worth adding the path of this data in our cluster to avoid useless downloading.\n",
    "\n",
    "In the following part we will show how to modify these variables. \n",
    "\n",
    "**To decide if you want to modify them you need to set the following variables to True.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01235f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CONFIGURE THE ESGF CATALOG ================ #\n",
    "\n",
    "### USE ALL NODES FOR SEARCH ###\n",
    "\n",
    "all_indices = True\n",
    "\n",
    "### SET A NEW DOWNLOADING PATH ###\n",
    "\n",
    "set_new_downloading_path = True\n",
    "\n",
    "### SET A CLUSTER SPECIFIC CMIP6 PATH ###\n",
    "\n",
    "cluster_local_CMIP6_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24386ed-c4b0-43fd-bb75-2ad48ef07bca",
   "metadata": {},
   "source": [
    "### Define the nodes for the research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732995b",
   "metadata": {},
   "source": [
    "We may decide to look at all the nodes. Note that the solr nodes are way much slower than the globus ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f5807-44b0-4152-af85-c23a574394ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_esgf.conf.set(all_indices=all_indices)\n",
    "\n",
    "if all_indices:\n",
    "\n",
    "    print(\"We are looking at all the nodes.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"We are only looking at the globus nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137085e-e87b-4539-b175-ce77c775195b",
   "metadata": {},
   "source": [
    "### Set where the data will be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35ff08-7788-4199-8596-f977b9f9c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_new_downloading_path:\n",
    "\n",
    "    intake_esgf.conf.set(local_cache=downloading_path)\n",
    "\n",
    "    print(\n",
    "        \"The CMIP6 data will be downloaded at the path : {}\".format(\n",
    "            intake_esgf.conf[\"local_cache\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4aaaf-d574-4f92-af6e-47c8d0673075",
   "metadata": {},
   "source": [
    "### Add a cluster-specific CMIP6 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c98fe2-3864-4373-a6cd-ebbc2dded7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_local_CMIP6_path != \"\":\n",
    "\n",
    "    intake_esgf.conf.set(esg_dataroot=cluster_local_CMIP6_path)\n",
    "\n",
    "    print(\n",
    "        \"The CMIP6 data will be searched beforehand at the path : {}\".format(\n",
    "            intake_esgf.conf[\"esg_dataroot\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No local cluster-specific CMIP6 path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d5d09",
   "metadata": {},
   "source": [
    "### Print the new configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intake_esgf.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a19cd6-d165-428f-984f-9133f7339dc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Make the query to the ESGFCatalog\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380bc91-38b4-4748-9041-5c43fe523818",
   "metadata": {},
   "source": [
    "### Initialise the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a145bbb-ad2d-4545-a8ed-625365507741",
   "metadata": {},
   "source": [
    "The catalog variable is initially empty and will be filled given the criterias that we will impose for the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d445ed-8924-4f62-ba7c-7e4644e0d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake_esgf.ESGFCatalog()\n",
    "\n",
    "print(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d3079-ebc4-41a0-aa62-38c62de2e50c",
   "metadata": {},
   "source": [
    "### Constrain the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25edd29",
   "metadata": {},
   "source": [
    "We apply the criterias defined earlier to the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ebabc-cb5d-48de-86d2-a45172e67696",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.search(**search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5087b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(catalog.session_log())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc077191",
   "metadata": {},
   "source": [
    "### Convert the catalog results into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b067741",
   "metadata": {},
   "source": [
    "The resulting catalog can be converted to a pandas dataframe. This is convenient to isolate some properties of the catalog like the models that it has found also known as the *source_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CONVERT TO PANDAS DATAFRAME ================ #\n",
    "\n",
    "initial_search_df = catalog.df\n",
    "\n",
    "### SHOW THE SOURCE_ID THAT APPEAR AT LEAST ONCE ###\n",
    "\n",
    "## Extract the list of the models' names ##\n",
    "\n",
    "# Retrieve the source_id column and take only one example for every duplicate #\n",
    "\n",
    "list_model_names = initial_search_df.source_id.unique()\n",
    "\n",
    "## Print the result ##\n",
    "\n",
    "print(\"The list of found models' names is : \\n{}\".format(list_model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2028de0",
   "metadata": {},
   "source": [
    "### Regroup the results by model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc6fb3",
   "metadata": {},
   "source": [
    "The number of found results is very large. They are numerous duplicates that come from the different *member_id*  and *grid_label* available for each model. \n",
    "\n",
    "The *member_id* or *variant_label* is described by 4 indices defining an ensemble member: *r* for realization, *i* for initialization, *p* for physics, and *f* for forcing. These parameters define an ensemble of experiments that correspond to the main experiment conditions for a given model. Actually, modellers may initialize their model from a different point in time, change the parametrization of a given parameter and so on.\n",
    "\n",
    "Let's regroup the results according to **(model, variant, grid)** tuples. The result here is truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_models = catalog.model_groups()\n",
    "\n",
    "print(grouped_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe084874",
   "metadata": {},
   "source": [
    "An interesting thing to note is the ultimate column that tells us the number of found results for a given model, variant and grid. In our case since we are looking for **8** variables for **two experiments**, the search will be deemed complete if we find **16 resulting files**.\n",
    "\n",
    "The next step is therefore to get rid of the incomplete results according to an user-defined criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62922b20",
   "metadata": {},
   "source": [
    "### Investigate for a specific model lacking variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05928fa",
   "metadata": {},
   "source": [
    "From what we have seen from the previous method, some models lack the expected number of variables. Thanks to the pandas' dataframe structure, we may investigate what are the variables missing for which experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ SEE WHAT VARIABLES ARE MISSING ================ #\n",
    "\n",
    "### LOOK AT A GIVEN MODEL ###\n",
    "\n",
    "## Define source_id and member_id ##\n",
    "\n",
    "# Source_id #\n",
    "\n",
    "looked_source_id = \"GFDL-ESM4\"\n",
    "\n",
    "# Member_id #\n",
    "\n",
    "looked_member_id = \"r1i1p1f1\"\n",
    "\n",
    "## Select the given source_id and member_id ##\n",
    "\n",
    "# Source_id #\n",
    "\n",
    "selected_source_id = initial_search_df[initial_search_df.source_id == looked_source_id]\n",
    "\n",
    "# Member_id #\n",
    "\n",
    "selected_model = selected_source_id[selected_source_id.member_id == looked_member_id]\n",
    "\n",
    "## Generate the variable_id series for the piClim-control experiment ##\n",
    "\n",
    "variable_id_for_control_exp = selected_model[selected_model.experiment_id == \"piClim-control\"].variable_id\n",
    "\n",
    "## Generate the variable_id series for the piClim-aer experiment ##\n",
    "\n",
    "variable_id_for_aer_exp = selected_model[selected_model.experiment_id == \"piClim-aer\"].variable_id\n",
    "\n",
    "### FIND VARIABLES THAT ARE MISSING FOR BOTH EXPERIMENTS ###\n",
    "\n",
    "## Make the criteria variable_id list as a panda series ##\n",
    "\n",
    "searched_variable_ids = pd.Series(variable_id, dtype = str)\n",
    "\n",
    "## Get the variables that are found in at least one experiment ##\n",
    "\n",
    "var_in_at_least_one = pd.Series(np.union1d(variable_id_for_control_exp, variable_id_for_aer_exp), dtype = str) \n",
    "\n",
    "## See the missing ones compared to the variable_id list ##\n",
    "\n",
    "print(\"Variables missing in both experiments of {}.{} : \\n\".format(looked_source_id, looked_member_id))\n",
    "\n",
    "missing_variables_from_both = searched_variable_ids[~searched_variable_ids.isin(var_in_at_least_one)].values\n",
    "\n",
    "print(missing_variables_from_both)\n",
    "\n",
    "### GET THE VARIABLES MISSING IN ONLY ONE EXPERIMENT ###\n",
    "\n",
    "## Extract the variables found in both experiments ##\n",
    "\n",
    "var_in_both = pd.Series(np.intersect1d(variable_id_for_control_exp, variable_id_for_aer_exp), dtype = str) \n",
    "\n",
    "## Get the variables not in common between the experiments ##\n",
    "\n",
    "# we keep the variables that are not in both experiments \n",
    "# by removing the ones that are found in both\n",
    "\n",
    "var_not_in_common = var_in_at_least_one[~var_in_at_least_one.isin(var_in_both)] \n",
    "\n",
    "## See the missing ones compared to the variable_id list ##\n",
    "\n",
    "missing_variables_not_in_common = searched_variable_ids[searched_variable_ids.isin(var_not_in_common)].values\n",
    "\n",
    "## Find the concerned experiment if missing_variables_not_in_common is not empty ##\n",
    "\n",
    "# Check if it's empty #\n",
    "\n",
    "if (missing_variables_not_in_common.size > 0) :\n",
    "\n",
    "    # Get the number of variables found for each experiment #\n",
    "\n",
    "    number_variables_control = len(variable_id_for_control_exp.values)\n",
    "\n",
    "    number_variables_aer = len(variable_id_for_aer_exp.values)\n",
    "\n",
    "    # Get the least furnished experiment #\n",
    "\n",
    "    if number_variables_control < number_variables_aer :\n",
    "\n",
    "        print(\"\\nOnly the control experiment of {}.{} is lacking these variables :\\n\".format(looked_source_id, looked_member_id))\n",
    "\n",
    "        print(missing_variables_not_in_common)\n",
    "\n",
    "    else :\n",
    "        \n",
    "        print(\"\\nOnly the aerosol experiment of {}.{} is lacking these variables :\\n\".format(looked_source_id, looked_member_id))\n",
    "\n",
    "        print(missing_variables_not_in_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90eaf6a",
   "metadata": {},
   "source": [
    "### Test some filters on the incomplete results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb2ee9",
   "metadata": {},
   "source": [
    "The user may define an expected number of netcdf files for a given (model, variant,grid) tuple. In our case this is **16** as explained before. Let's impose this result by keeping only the results that match this condition. We will produce a panda series that will allow us to check if our filter is what we want.\n",
    "\n",
    "You may define a more refined filter for the results. Please look at the intake-esgf documentation for more information : https://intake-esgf.readthedocs.io/en/latest/modelgroups.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ TEST A FILTER BY GROUPING BY MODELS ================ #\n",
    "\n",
    "### SET THE EXPECTED NUMBER OF FILES ###\n",
    "\n",
    "expected_number_of_files = 16\n",
    "\n",
    "### FILTER THE INCOMPLETE RESULTS ACCORDING TO OUR CRITERIE ###\n",
    "\n",
    "filtered_results = grouped_models[grouped_models == expected_number_of_files]\n",
    "\n",
    "### PRINT THE FILTERED CATALOG ###\n",
    "\n",
    "print(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2be0a",
   "metadata": {},
   "source": [
    "The result is a pandas series that we can manipulate as so. For example if we want to retrieve all the member_id and grid_label associated to a single modell like the *IPSL-CM6A-LR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e56605",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results[\"IPSL-CM6A-LR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d67fe",
   "metadata": {},
   "source": [
    "Good, let's have a look at the quantity of results we have left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The number of remaining (model, variant, grid) tuples is {}.\".format(\n",
    "        filtered_results.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980174f",
   "metadata": {},
   "source": [
    "### Applying the filter on the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9c50d",
   "metadata": {},
   "source": [
    "If that's satisfy us, we need to code a small function that will be **applied to the catalog**. The test will be executed on each model group that we have showed in the previous section. If we wish to reproduce the results from *Zelinka and al (2023)*, we can also keep only the matching models and variant couples with a the provided list in the article.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Zelinka, M. D., Smith, C. J., Qin, Y., and Taylor, K. E.: Comparison of methods to estimate aerosol effective radiative forcings in climate models, Atmos. Chem. Phys., 23, 8879â€“8898, https://doi.org/10.5194/acp-23-8879-2023, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINE THE MODEL.VARIANT LIST OF ZELINKA'S ARTICLE ================ #\n",
    "\n",
    "source_id_zelinka_2023 = [\n",
    "    \"ACCESS-CM2\",\n",
    "    \"ACCESS-ESM1-5\",\n",
    "    \"BCC-ESM1\",\n",
    "    \"CESM2\",\n",
    "    \"CNRM-CM6-1\",\n",
    "    \"CNRM-ESM2-1\",\n",
    "    \"CanESM5\",\n",
    "    \"GFDL-CM4\",\n",
    "    \"GFDL-ESM4\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"HadGEM3-GC31-LL\",\n",
    "    \"IPSL-CM6A-LR-INCA\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"MIROC6\",\n",
    "    \"MIROC6\",\n",
    "    \"MPI-ESM-1-2-HAM\",\n",
    "    \"MRI-ESM2-0\",\n",
    "    \"NorESM2-LM\",\n",
    "    \"NorESM2-LM\",\n",
    "    \"NorESM2-MM\",\n",
    "    \"UKESM1-0-LL\" \n",
    "]\n",
    "\n",
    "member_id_zelinka_2023 = [\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p2f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p3f1\",\n",
    "    \"r1i1p1f3\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r2i1p1f1\",\n",
    "    \"r3i1p1f1\",\n",
    "    \"r4i1p1f1\",\n",
    "    \"r11i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p2f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f4\"\n",
    "]\n",
    "\n",
    "zelinka_2023_model_variant_table = pd.DataFrame({\"source_id\" : source_id_zelinka_2023, \"member_id\" : member_id_zelinka_2023}, dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINING THE FILTERING FUNCTION ================ #\n",
    "\n",
    "### SET THE EXPECTED NUMBER OF FILES ###\n",
    "\n",
    "expected_number_of_files = 16\n",
    "\n",
    "### DEFINE THE GLOBAL OPTIONAL PARAMETERS OF THE FILTERING FUNCTION ###\n",
    "\n",
    "filtering_by_name = True\n",
    "\n",
    "keep_only_dataframe = zelinka_2023_model_variant_table\n",
    "\n",
    "### DEFINE THE FILTERING FUNCTION ###\n",
    "\n",
    "def filtering_function(grouped_model_entry : pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINITION ###\n",
    "\n",
    "    This function allows the intake-esgf catalog to be cleaned of the entries that are not complete.\n",
    "    In the default case it means entries that do not meet the expected number of files. \n",
    "\n",
    "    The user can also set a condition that only the model and variant couples present in a provided pandas dataframe are kept.\n",
    "    Since the nature of this function is to be an input for the intake-esgf package, we define the optional arguments outside\n",
    "    of the function.\n",
    "\n",
    "    ### INPUTS\n",
    "\n",
    "    GROUPED_MODEL_ENTRY : Pandas DataFrame | sub dataframe containing all the variables of a given source_id, member_id and grid tuple.\n",
    "\n",
    "    ### OPTIONAL ARGUMENTS (DEFINED GLOBALLY)\n",
    "\n",
    "    FILTERING_BY_NAME : BOOL | defines if we filter the entry by source_id and member_id or not\n",
    "\n",
    "    KEEP_ONLY_DATAFRAME : Pandas DataFrame | associated dataframe holding the source_id and member_id to conserve\n",
    "    \n",
    "    ### OUTPUTS\n",
    "\n",
    "    BOOL | whether we keep this model group or not\n",
    "    \"\"\"\n",
    "\n",
    "    ### TEST THE NUMBER OF VARIBALES ###\n",
    "\n",
    "    if len(grouped_model_entry) == expected_number_of_files:\n",
    "\n",
    "        ### NUMBER OF VARIABLES' TEST SUCCEEDED ###\n",
    "\n",
    "        ## Do we keep only the Zelinka's article model and variant couples ? ##\n",
    "\n",
    "        # NO : We keep everything that matched the variable number's test #\n",
    "\n",
    "        if not(filtering_by_name):\n",
    "\n",
    "            return True\n",
    "        \n",
    "        ### YES : KEEPING ONLY THE COUPLES PRESENT IN ZELINKA 2023 ###\n",
    "\n",
    "        else :\n",
    "            \n",
    "            ## Doing the test on the grouped_model_entry's source_id and member_id ##\n",
    "\n",
    "            # Extract the grouped_model_entry data #\n",
    "\n",
    "            grouped_model_entry_source_id = grouped_model_entry.source_id.unique()[0]\n",
    "\n",
    "            grouped_model_entry_member_id = grouped_model_entry.member_id.unique()[0]\n",
    "\n",
    "            # Can we find the grouped_model_entry's source_id and member_id in one row of is_in_keep_only_dataframe ? #\n",
    "\n",
    "            is_in_keep_only_dataframe = ((keep_only_dataframe['source_id'] == grouped_model_entry_source_id) \n",
    "            & (keep_only_dataframe['member_id'] == grouped_model_entry_member_id)).any()\n",
    "\n",
    "            # Return the result of the test #\n",
    "\n",
    "            return is_in_keep_only_dataframe\n",
    "    \n",
    "    ### NUMBER OF VARIABLES' TEST FAILED ###\n",
    "\n",
    "    else :\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08053c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.remove_incomplete(filtering_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f22bd4",
   "metadata": {},
   "source": [
    "Looking at the resulting dataframe we see that we have indeed filtered the unwanted entries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7907795",
   "metadata": {},
   "source": [
    "## Downloading the files and load a dictionary in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e5884",
   "metadata": {},
   "source": [
    "The intake-esgf library proposes to store the found results in memory under the form of a dictionary holding xarray datasets for every single netcdf file found. This process also saves the dictionary at the previously defined local_cache path. \n",
    "\n",
    "By default, the package is looking for the **areacella** variable automatically. But it does it **rather slowly** and does not look first at what is stored on the local_cache. In our analysis, we would rather load the dictionary with *add_measures* set to **False** and then download (and load) the areacella netcdf files apart with a homemade routine. An example of this process is given afterward.\n",
    "\n",
    "What's more, we may describe some facets as irrelevant to build the keys of the dictionary. Indeed, by default, the intake_esgf package will build keys out of the facet values that are different among the entries in the output dictionary. But, some of these facets might not be of any interest for our analysis and we can drop them with the *ignore_facet*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371e2d7",
   "metadata": {},
   "source": [
    "### Prepare the download a lot of entries from the solr nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ab6b0",
   "metadata": {},
   "source": [
    "There is an issue that may be encountered if, as in this example, the user tried to download a lot of different entries from the solr index nodes. Indeed, the downloading of some variables of the data present on these nodes will fail. A solution that has been found in this case is to make a request for a **(source_id, member_id, grid_label)** tuple at once. We use for that two functions :\n",
    "\n",
    "- generate_single_model_search_criterias : which generates search_criterias for a single entry of grouped_models_dataframe precising \n",
    "the (source_id, member_id, grid_label) criterias needed to download only one entry.\n",
    "\n",
    "- update_single_entry_keys : which updates the single entry keys of the downloaded one entry dictionary to allow to concatenate all dictionaries together.\n",
    "\n",
    "We are able to generate a dictionnary holding all entries even though we download them separatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f532d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ FUNCTION TO DOWNLOAD ONE ENTRY AT A TIME ================ #\n",
    "\n",
    "def generate_single_model_search_criterias(search_criterias : dict,\n",
    "                              grouped_models_dataframe : pd.DataFrame, \n",
    "                              index : int,\n",
    "                              ) -> tuple[dict, str] :\n",
    "    \n",
    "    \"\"\"\n",
    "    ---\n",
    "\n",
    "    ### DEFINITION ###\n",
    "\n",
    "    This function generates search_criterias for a single entry of grouped_models_dataframe precising \n",
    "    the (source_id, member_id, grid_label) criterias needed to download only one entry.\n",
    "    ---\n",
    "\n",
    "    ### INPUTS ###\n",
    "\n",
    "    SEARCH_CRITERIAS :  DICT | the original search criterias dictionary \n",
    "\n",
    "    GROUPED_MODELS_DATAFRAME : PANDAS DATAFRAME | the entries dels that have been grouped together by source_id, member_id and grid_label\n",
    "\n",
    "    INDEX : INT | selected row index\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### OUTPUTS ###\n",
    "\n",
    "    SEARCH_CRITERIAS_GIVEN_ROW : dictionary | search criterias updated with the source_id, member_id and grid_label criterias of the given row.\n",
    "\n",
    "    SINGLE_MODEL_NAME : str | name of the single model we set the download for\n",
    "    ---\n",
    "\n",
    "    \"\"\"\n",
    "     \n",
    "    ### COPYING THE ORIGINAL SEARCH CRITERIAS dictionary ###\n",
    "\n",
    "    search_criterias_given_row = search.copy()\n",
    "    \n",
    "    ## Get the row information ##\n",
    "\n",
    "    # Source_id #\n",
    "\n",
    "    source_id_to_download = grouped_models_dataframe.iloc[index].source_id\n",
    "\n",
    "    # Member_id #\n",
    "\n",
    "    member_id_to_download = grouped_models_dataframe.iloc[index].member_id\n",
    "\n",
    "    # Grid_label #\n",
    "\n",
    "    grid_label_to_download = grouped_models_dataframe.iloc[index].grid_label\n",
    "\n",
    "    ## Update the search criterias ##\n",
    "\n",
    "    # Source_id #\n",
    "\n",
    "    search_criterias_given_row[\"source_id\"] = source_id_to_download\n",
    "\n",
    "    # Member_id #\n",
    "\n",
    "    search_criterias_given_row[\"member_id\"] = member_id_to_download\n",
    "\n",
    "    # Grid_label #\n",
    "\n",
    "    search_criterias_given_row[\"grid_label\"] = grid_label_to_download\n",
    "\n",
    "    ## Generate the single model name ##\n",
    "\n",
    "    single_model_name = source_id_to_download + \".\" + member_id_to_download + \".\" + grid_label_to_download\n",
    "    \n",
    "    return search_criterias_given_row, single_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ FUNCTION TO UPDATE THE KEYS OF THE DOWNLOADED ONE ENTRY DICTIONARY ================ #\n",
    "\n",
    "def update_single_entry_keys(\n",
    "        single_model_dictionary : dict,\n",
    "        single_model_name : str,\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    ---\n",
    "\n",
    "    ### DEFINITION ###\n",
    "\n",
    "    This function updates the single entry keys of the downloaded one entry dictionary to allow to concatenate all dictionaries together :\n",
    "\n",
    "    (experiment.variable) -> (source_id.member_id.grid_label.experiment.variable)\n",
    "    ---\n",
    "\n",
    "    ### INPUTS ###\n",
    "\n",
    "    SINGLE_MODEL_DICTIONARY :  DICT | the downloaded single_model_dictionary with its keys under the form (experiment.variable)\n",
    "\n",
    "    SINGLE_MODEL_NAME : STR | the name of the downloaded model under the form (source_id.member_id.grid_label)\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### OUTPUTS ###\n",
    "\n",
    "    SINGLE_MODEL_DICTIONARY : DICT | the updated single_model_dictionary with its keys under the form (source_id.member_id.grid_label.experiment.variable)\n",
    "    ---\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### INITIALIZE ###\n",
    "\n",
    "    ## Extract the dictionary keys ##\n",
    "\n",
    "    # We transform the keys into a list such that old_keys do not change during the loop #\n",
    "\n",
    "    old_keys = list(single_model_dictionary.keys())\n",
    "\n",
    "    ### UPDATING THE KEYS ###\n",
    "\n",
    "    for experiment_dot_variable in old_keys :\n",
    "\n",
    "        ## Changing from (experiment.variable) to (source_id.member_id.grid_label.experiment.variable) ##\n",
    "\n",
    "        new_key = single_model_name + \".\" + experiment_dot_variable\n",
    "\n",
    "        ## Updating the keys ##\n",
    "\n",
    "        single_model_dictionary[new_key] = single_model_dictionary.pop(experiment_dot_variable)\n",
    "\n",
    "    return single_model_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48dca1",
   "metadata": {},
   "source": [
    "### Download all the entries and save them into a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ EXTRACT THE FILTERED CATALOG'S INFORMATION ================ #\n",
    "\n",
    "### GET THE INFORMATIONS THAT THE CATALOG EXTRACTED ###\n",
    "\n",
    "## Generate the full dataframe of the files found by the search ##\n",
    "\n",
    "selected_entries_full_dataframe = catalog.df\n",
    "\n",
    "## Save the grouped model pandas series for the next part ##\n",
    "\n",
    "series_grouped_models = catalog.model_groups()\n",
    "\n",
    "### GET ONLY THE NEEDED INFORMATION ###\n",
    "\n",
    "## We extract the (source_id, member_id, grid_label) tuples from the full dataframe ##\n",
    "\n",
    "# we remove the duplicates to only keep one row per tuple\n",
    "\n",
    "grouped_models_dataframe = selected_entries_full_dataframe[[\"source_id\",\"member_id\",\"grid_label\"]].drop_duplicates().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DOWNLOAD EVERY SINGLE OUTPUT ================ #\n",
    "\n",
    "### DOWNLOAD EVERY SINGLE ENTRY AND COMBINE THEM INTO A DICTIONARY ###\n",
    "\n",
    "## Initialize the full dictionary ##\n",
    "\n",
    "full_cmip6_dict = {}\n",
    "\n",
    "## Downloading all the models one entry at a time ##\n",
    "\n",
    "print(\"Downloading and/or loading the data one entry at a time...\\n\")\n",
    "\n",
    "for index in grouped_models_dataframe.index :\n",
    "\n",
    "    ## Reset the catalog ##\n",
    "\n",
    "    catalog = intake_esgf.ESGFCatalog()\n",
    "\n",
    "    ## Generate the associated search criterias ##\n",
    "    \n",
    "    search_criterias_given_row, single_model_name = generate_single_model_search_criterias(\n",
    "        search_facets = search, \n",
    "        grouped_models_dataframe = grouped_models_dataframe,\n",
    "        index = index\n",
    "        )\n",
    "    \n",
    "    ## Generate the single model's output name ##\n",
    "\n",
    "    print(\"\\nDownloading {} ...\\n\".format(single_model_name))\n",
    "\n",
    "    ## Apply the search criterias ##\n",
    "\n",
    "    catalog.search(\n",
    "            **search_criterias_given_row,\n",
    "        )\n",
    "\n",
    "    ## Downloading the output... ##\n",
    "\n",
    "    single_model_dictionary = catalog.to_dataset_dict(\n",
    "            add_measures=False,\n",
    "            ignore_facets=[\"project\",\"mip_era\",\"activtity_drs\", \"institution_id, table_id\",\"grid_label\",\"version\"],\n",
    "            quiet=True,\n",
    "        )\n",
    "    \n",
    "    ## Updating its keys ##\n",
    "\n",
    "    single_model_dictionary = update_single_entry_keys(single_model_dictionary, single_model_name)\n",
    "\n",
    "    ## Updating the full dictionary ##\n",
    "\n",
    "    full_cmip6_dict = full_cmip6_dict | single_model_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ebc1e",
   "metadata": {},
   "source": [
    "## Homemade routine to retrieve the areacella of a given model, variant, grid_label tuple only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1398b16",
   "metadata": {},
   "source": [
    "The main default of the intake-esgf package when it comes to look for measures is that it does so for every single variable. And this is independent of the fact that our 12 variables fore one experiment would share the same areacella grid. What's more, it always start by looking for our full set of facets which is irrevelant. \n",
    "\n",
    "Indeed, the only three parameters that define areacella are the **source_id**, **member_id** and obviously the **grid_label**. Conveniently enough, we can group our search results according to these three parameters with the *.model_groups* method. In the end, it may be that our experiment won't hold the areacella variable but it does not really matter, we just need to find one. This is the spirit of this routine made to still get areacella but significantly faster.\n",
    "\n",
    "It is worth noting that this method is not useful if you need to import a small number of models and variables as you won't really see the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e379d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5877905",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_esgf.conf.set(all_indices=all_indices)\n",
    "\n",
    "if all_indices:\n",
    "\n",
    "    print(\"We are looking at all the nodes.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"We are only looking at the globus nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff61345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ HOMEMADE ROUTINE TO GET AREACELLA GRIDS FASTER ================ #\n",
    "\n",
    "### INITIALISATION  ###\n",
    "\n",
    "## Initialise the full dictionary ##\n",
    "\n",
    "dict_areacella = {}\n",
    "\n",
    "## Number of rows ##\n",
    "\n",
    "n_rows = grouped_models.size\n",
    "\n",
    "### GO THROUGH EVERY ROW OF THE PANDA SERIES ###\n",
    "\n",
    "for ii in range(n_rows):\n",
    "\n",
    "    ## Get the SOURCE_ID | MEMBER_ID | GRID_LABEL of the row ##\n",
    "\n",
    "    # Retrieve the row ##\n",
    "\n",
    "    row_ii = grouped_models.index[ii]\n",
    "\n",
    "    # Extract the labels #\n",
    "\n",
    "    source_id, member_id, grid_label = row_ii\n",
    "\n",
    "    # Build the key for this dictionary entry ##\n",
    "\n",
    "    full_key = source_id + \".\" + member_id + \".\" + grid_label\n",
    "\n",
    "    ## Special case for the IPSL-CM6A-LR-INCA model ##\n",
    "\n",
    "    if source_id == \"IPSL-CM6A-LR-INCA\":\n",
    "\n",
    "        source_id = \"IPSL-CM6A-LR\"\n",
    "\n",
    "    ## Do the full search ##\n",
    "\n",
    "    areacella_search_full = catalog.search(\n",
    "        source_id=source_id,\n",
    "        grid_label=grid_label,\n",
    "        variable_id=\"areacella\",\n",
    "        quiet=True,\n",
    "    ).df  # silence the progress bar\n",
    "\n",
    "    ## Extract the first experiment id that gives an areacella entry ##\n",
    "\n",
    "    only_first_exp_id = areacella_search_full.experiment_id.values[0]\n",
    "\n",
    "    ## Extract the first member id that gives an areacella entry ##\n",
    "\n",
    "    only_first_member_id = areacella_search_full.member_id.values[0]\n",
    "\n",
    "    ## Get the areacella for the given row ##\n",
    "\n",
    "    # Search and download it #\n",
    "\n",
    "    print(\"\\nDownloading areacella for {} ...\\n\".format(full_key))\n",
    "\n",
    "    areacella_ii = catalog.search(\n",
    "        source_id=source_id,\n",
    "        grid_label=grid_label,\n",
    "        variable_id=\"areacella\",\n",
    "        experiment_id=only_first_exp_id,\n",
    "        member_id=only_first_member_id,\n",
    "        quiet=True\n",
    "    ).to_dataset_dict(\n",
    "        add_measures=False, \n",
    "        quiet=True\n",
    "    )  # silence the progress bar\n",
    "\n",
    "    # Store it in dictionary #\n",
    "\n",
    "    dict_areacella[full_key] = areacella_ii[\"areacella\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmip6-download",
   "language": "python",
   "name": "cmip6-download"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
