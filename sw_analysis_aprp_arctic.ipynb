{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP6 Arctic Case Study : Compute the aerosol-cloud interaction effective radiative forcing\n",
    "\n",
    "--- \n",
    "\n",
    "## What this notebook holds :\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:red\">**It is recommended to first run the *get_cmip6_data.ipynb* notebook to download and prepare the raw model's outputs data for the analysis.**</span>\n",
    "\n",
    "This notebook aims at showing the effective radiative forcing that is caused by the aerosol burdens' levels of 2014 in the shortwabe. It aims at evaluating the different components of the aerosol-cloud's interaction impact on the Arctic warming. This is done using a subset of the CMIP6 model ensemble and using the APRP method devised by *Taylor and al. (2007)* and whose implementation was done by *Zelinka and al. (2023)*. \n",
    "\n",
    "The subset used is described in the github repository and is the one used by *Zelinka and al.*. It can be found in details in the *load_cmip6* submodule as the case described by *ZELINKA-SW* in the *set_search_criterias* function. \n",
    "\n",
    "### The use of CMIP6 data\n",
    "\n",
    "We use two experiments realized during the CMIP6  : **piClim-control** and **piClim-aer**. These are both atmosphere-only climate model simulations in which sea surface temperatures (SSTs) and sea icea concentrations (SICs) are fixed at model-specific preindustrial climatological values. The description of the experiments can be found here : https://wcrp-cmip.github.io/CMIP6_CVs/docs/CMIP6_experiment_id.html. On the one hand, the **piClim-control** realization assumes aerosols burdens set to their preindustrial levels hence why it is dubbed as the control experiment. On the other hand, the **piClim-aer** realization uses present-day, present-day being 2014, aerosols burdens' levels.\n",
    "\n",
    "The variable used are listed and explicited below according to : https://clipc-services.ceda.ac.uk/dreq/mipVars.html. All the variables are monthly timeseries over 30 years. We use the monthly climatology of each of these variables.\n",
    "\n",
    "> <span style=\"color:SkyBlue\">**clt**</span>  : Total cloud area fraction (%) for the whole atmospheric column\n",
    ">\n",
    "> <span style=\"color:gold\">**rsdt**</span> : Shortwave radiation ($W/m^{2}$) **incident** at the TOA\n",
    "> \n",
    "> <span style=\"color:orange\">**rsut**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at the TOA\n",
    ">\n",
    "> <span style=\"color:orangered\">**rsutcs**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at TOA for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:Orchid\">**rsds**</span> : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface\n",
    "> \n",
    "> <span style=\"color:Indigo \">**rsdscs**</span>  : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:YellowGreen\">**rsus**</span> : Shortwave **upwelling** radiation ($W/m^{2}$) at the surface\n",
    ">\n",
    "> <span style=\"color:Darkgreen\">**rsuscs**</span>: Shortwave **upwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    ">\n",
    "> **areacella** : For every grid, the latitude-dependent surface associated to each grid point.\n",
    "\n",
    "### Approximate Partial Radiative Perturbation\n",
    "\n",
    "**TO WRITE...**\n",
    "\n",
    "## References : \n",
    "\n",
    "### APRP method\n",
    "\n",
    "Taylor, K. E., M. Crucifix, P. Braconnot, C. D. Hewitt, C. Doutriaux, A. J. Broccoli, J. F. B. Mitchell, and M. J. Webb, 2007: Estimating Shortwave Radiative Forcing and Response in Climate Models. J. Climate, 20, 2530–2543, https://doi.org/10.1175/JCLI4143.1. \n",
    "\n",
    "### APRP module used for our analysis\n",
    "\n",
    "https://github.com/mzelinka/aprp\n",
    "\n",
    "Zelinka, M. D., Smith, C. J., Qin, Y., and Taylor, K. E.: Comparison of methods to estimate aerosol effective radiative forcings in climate models, Atmos. Chem. Phys., 23, 8879–8898, https://doi.org/10.5194/acp-23-8879-2023, 2023.\n",
    "\n",
    "### Author\n",
    "\n",
    "Giboni Lucas, 1st year PhD Candidate at IGE (CNRS), Grenoble.\n",
    "\n",
    "https://github.com/gibonil/gibonil\n",
    "\n",
    "### License\n",
    "\n",
    "Feel free to share, use and improve the following code according to the provided license on the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Initialisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ IMPORTATIONS ================ #\n",
    "\n",
    "### LOAD AND NAVIGATE THROUGH THE DATA ###\n",
    "\n",
    "import os  # to get access to commands related to path setting and creation of directories\n",
    "\n",
    "### DATA OBJECTS AND ASSOCIATED COMPUTATION ###\n",
    "\n",
    "import numpy as np  # to handle numpy arrays and the associated tools\n",
    "\n",
    "import xarray as xr  # to manage the data\n",
    "\n",
    "import xcdat as xc  # to handle climate model outputs with xarray\n",
    "\n",
    "### REPRESENTING DATA ###\n",
    "\n",
    "import matplotlib.pyplot as plt  # to handle plotting routines\n",
    "\n",
    "import cartopy.crs as ccrs  # to handle map projections\n",
    "\n",
    "### HOMEMADE LIBRARIES ###\n",
    "\n",
    "## Load the climatology dictionary ##\n",
    "\n",
    "from utilities.get_cmip6_data.store_data.dict_netcdf_transform import (\n",
    "    netcdf_to_dict, # to load the climatology dictionary\n",
    ")  \n",
    "\n",
    "## Handle the climatology dictionary ##\n",
    "\n",
    "from utilities.get_cmip6_data.prepare_data.extract_climatologies import (\n",
    "    get_entries_only_from_clim_dict, # to extract the keys without the experiments,\n",
    "    add_one_variable_to_dataset, # to add one variable to the full dataset\n",
    ")\n",
    "\n",
    "## Generate a common grid on which we will regrid all CMIP6 outputs ##\n",
    "\n",
    "from utilities.regridding.regridding_methods import(\n",
    "    generate_the_common_coarse_grid, # to create the common coarse grid \n",
    ")\n",
    "### APRP LIBRARY ###\n",
    "\n",
    "from utilities.aprp.code.aprp import(\n",
    "    APRP, # APRP method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the paths for saving and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the user needs to define the paths at which will be downloaded the data and saved the climatologies if necessary. They also need to define the path to the table associating the monthly climatologies netcdf files with their respective key in the climatologies' dictionary to be loaded. These paths are the absolute paths from the home directory.\n",
    "\n",
    "If *get_cmip6_data.ipynb* notebook was run before, then only the path to the *key_paths_table.pkl* file is relevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINE THE FOLDERS WHERE IS STORED THE DATA ================ #\n",
    "\n",
    "### DEFINE THE HOME DIRECTORY ###\n",
    "\n",
    "## Home directory ##\n",
    "\n",
    "homedir_path = os.path.expanduser(\"~\")\n",
    "\n",
    "### DEFINE WHERE TO DOWNLOAD THE RAW DATA ###\n",
    "\n",
    "## Parent directory of the download folder ##\n",
    "\n",
    "parent_path_download = homedir_path + \"/certainty-data\"\n",
    "\n",
    "## Name of the download folder ##\n",
    "\n",
    "download_folder_name = \"CMIP6-DATA\"\n",
    "\n",
    "### DEFINE WHERE TO SAVE THE CLIMATOLOGIES ###\n",
    "\n",
    "## Path of the save directory ##\n",
    "\n",
    "parent_path_save_clim = (\n",
    "    homedir_path + \"/certainty-data/\" + download_folder_name + \"/climatologies\"\n",
    ")\n",
    "\n",
    "### DEFINE WHERE TO LOOK FOR THE TABLE OF THE CLIMATOLOGIES' PATHS ###\n",
    "\n",
    "table_path = parent_path_save_clim + \"/table\" + \"/key_paths_table.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CMIP6 climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the CMIP6 climatology. We first check the existence of the *key_paths_table.pkl* file that indicate that the *get_cmip6_data.ipynb* notebook was successfully run. Otherwise, we launch the full routine to download the raw data and generate the climatologies. \n",
    "\n",
    "**This full routine lasts about 2 hours if no download was done before.**\n",
    "\n",
    "Sometimes the data cannot be found for the downloading phase even though the entry was found in the search phase... The user will need to relaunch the code as it is an error linked to the connection to the servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monthly climatologies were generated and can be retrieved.\n"
     ]
    }
   ],
   "source": [
    "# ================ LOAD THE MONTHLY CLIMATOLOGIES ================ #\n",
    "\n",
    "### CHECK IF THE CLIMATOLOGIES WERE GENERATED BEFORE ###\n",
    "\n",
    "if os.path.lexists(path = table_path) :\n",
    "\n",
    "    ## The table exists we can therefore load the dictionary of the climatologies ## \n",
    "\n",
    "    print(\"The monthly climatologies were generated and can be retrieved.\")\n",
    "\n",
    "    data_cmip6_clim = netcdf_to_dict(parent_path_for_save = parent_path_save_clim)\n",
    "\n",
    "else :\n",
    "\n",
    "    ## The table does not exist : we need to download the data and prepare it ##\n",
    "\n",
    "    print(\"No key_paths_table.pkl at the given path.\\n\" \\\n",
    "    \"We download the data and create the monthly climatologies dictionary.\")\n",
    "\n",
    "    # We import the necessary submodule #\n",
    "\n",
    "    from utilities.get_cmip6_data.prepare_data.extract_climatologies import (\n",
    "    create_climatology_dict,  # to create the climatology dictionary and save it\n",
    "    )\n",
    "\n",
    "    ## Create the climatologies dictionary ##\n",
    "\n",
    "    create_climatology_dict(\n",
    "        data_path = parent_path_download,\n",
    "        data_folder_name = download_folder_name,\n",
    "        parent_path_for_save = parent_path_save_clim,\n",
    "        selected_case = \"ZELINKA-SW\",\n",
    "        do_we_clear = True,\n",
    "        verbose = False,\n",
    "    )\n",
    "\n",
    "    ## Load it ##\n",
    "\n",
    "    data_cmip6_clim = netcdf_to_dict(parent_path_for_save = parent_path_save_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Approximate Partial Radiative Perturbation computation to our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the APRP dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the APRP function we be able to call both the **control** and **aer** xarray datasets for a given entry. This is why we start by generating the list of the dictionary keys without the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ GET THE KEYS WITHOUT THE EXPERIMENT REFERENCE ================ #\n",
    "\n",
    "### GENERATE THE KEYS WITHOUT THE EXPERIMENT ###\n",
    "\n",
    "## Copy the list of the keys of the clim dictionary ##\n",
    "\n",
    "clim_keys = list(data_cmip6_clim.keys())\n",
    "\n",
    "## Generate the unique list of keys without the reference to the experiments ##\n",
    "\n",
    "clim_keys_without_exp = [\n",
    "    get_entries_only_from_clim_dict(key_with_exp) for key_with_exp in clim_keys\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the APRP method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_aprp = {}\n",
    "\n",
    "for key in clim_keys_without_exp:\n",
    "\n",
    "    key_control = key + \".piClim-control\"\n",
    "\n",
    "    key_aer = key + \".piClim-aer\"\n",
    "\n",
    "    data_cmip6_clim[key_control].load()\n",
    "\n",
    "    data_cmip6_clim[key_aer].load()\n",
    "\n",
    "    output = APRP(data_cmip6_clim[key_control], data_cmip6_clim[key_aer])\n",
    "\n",
    "    dict_aprp[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "dict_aprp = {\n",
    "    key: dict_aprp[key]\n",
    "    .groupby(dict_aprp[key].time.dt.month)\n",
    "    .mean()\n",
    "    for key in keys_dict_aprp\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve the source_ids\n",
    "\n",
    "go through them\n",
    "\n",
    "for each source_id get the corresponding indexes and keep only the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCESS-CM2.r1i1p1f1.gn',\n",
       " 'ACCESS-ESM1-5.r1i1p1f1.gn',\n",
       " 'BCC-ESM1.r1i1p1f1.gn',\n",
       " 'CESM2.r1i1p1f1.gn',\n",
       " 'CNRM-CM6-1.r1i1p1f2.gr',\n",
       " 'CNRM-ESM2-1.r1i1p1f2.gr',\n",
       " 'CanESM5.r1i1p2f1.gn',\n",
       " 'GFDL-CM4.r1i1p1f1.gr1',\n",
       " 'GISS-E2-1-G.r1i1p1f1.gn',\n",
       " 'GISS-E2-1-G.r1i1p1f2.gn',\n",
       " 'GISS-E2-1-G.r1i1p3f1.gn',\n",
       " 'HadGEM3-GC31-LL.r1i1p1f3.gn',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r2i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r3i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r4i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR-INCA.r1i1p1f1.gr',\n",
       " 'MIROC6.r11i1p1f1.gn',\n",
       " 'MIROC6.r1i1p1f1.gn',\n",
       " 'MPI-ESM-1-2-HAM.r1i1p1f1.gn',\n",
       " 'MRI-ESM2-0.r1i1p1f1.gn',\n",
       " 'NorESM2-LM.r1i1p1f1.gn',\n",
       " 'NorESM2-LM.r1i1p2f1.gn',\n",
       " 'NorESM2-MM.r1i1p1f1.gn',\n",
       " 'UKESM1-0-LL.r1i1p1f4.gn']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_keys_list = list(keys_dict_aprp)\n",
    "\n",
    "dict_keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = list(keys_dict_aprp)\n",
    "\n",
    "def extract_source_id_from_str(key : str) -> str :\n",
    "\n",
    "        splitted_key = key.split(\".\")\n",
    "\n",
    "        source_id = splitted_key[0]\n",
    "\n",
    "        return source_id\n",
    "\n",
    "def get_source_id_from_keys_list(keys_list : list[str]) -> list[str] :\n",
    "        \n",
    "        source_id_list = [extract_source_id_from_str(key) for key in keys_list]\n",
    "        \n",
    "        return source_id_list\n",
    "\n",
    "def find_first_index_for_given_source_id(given_source_id : str, source_id_list : list[str]) -> int :\n",
    "\n",
    "        index_list = [i for i, source_id in enumerate(source_id_list) if source_id == given_source_id]\n",
    "\n",
    "        return index_list[0]\n",
    "\n",
    "def extract_only_one_variant_keys_list(keys_list : list[str]) -> list[str]:\n",
    "\n",
    "        source_id_list = get_source_id_from_keys_list(keys_list)\n",
    "\n",
    "        unique_source_id_list = np.unique(source_id_list)\n",
    "\n",
    "        index_to_keep = [find_first_index_for_given_source_id(given_source_id = given_source_id, source_id_list = source_id_list) for given_source_id in unique_source_id_list]\n",
    "\n",
    "        only_one_variant_keys_list = [keys_list[index] for index in index_to_keep]\n",
    "\n",
    "        return only_one_variant_keys_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCESS-CM2.r1i1p1f1.gn',\n",
       " 'ACCESS-ESM1-5.r1i1p1f1.gn',\n",
       " 'BCC-ESM1.r1i1p1f1.gn',\n",
       " 'CESM2.r1i1p1f1.gn',\n",
       " 'CNRM-CM6-1.r1i1p1f2.gr',\n",
       " 'CNRM-ESM2-1.r1i1p1f2.gr',\n",
       " 'CanESM5.r1i1p2f1.gn',\n",
       " 'GFDL-CM4.r1i1p1f1.gr1',\n",
       " 'GISS-E2-1-G.r1i1p1f1.gn',\n",
       " 'HadGEM3-GC31-LL.r1i1p1f3.gn',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR-INCA.r1i1p1f1.gr',\n",
       " 'MIROC6.r11i1p1f1.gn',\n",
       " 'MPI-ESM-1-2-HAM.r1i1p1f1.gn',\n",
       " 'MRI-ESM2-0.r1i1p1f1.gn',\n",
       " 'NorESM2-LM.r1i1p1f1.gn',\n",
       " 'NorESM2-MM.r1i1p1f1.gn',\n",
       " 'UKESM1-0-LL.r1i1p1f4.gn']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_only_one_variant_keys_list(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 23,\n",
       " 24]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_list = get_source_id_from_keys_list(keys_list)\n",
    "\n",
    "index_to_keep = [find_first_index_for_given_source_id(given_source_id = given_source_id, source_id_list = source_id_list) for given_source_id in source_id_list]\n",
    "\n",
    "index_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCESS-CM2.r1i1p1f1.gn',\n",
       " 'ACCESS-ESM1-5.r1i1p1f1.gn',\n",
       " 'BCC-ESM1.r1i1p1f1.gn',\n",
       " 'CESM2.r1i1p1f1.gn',\n",
       " 'CNRM-CM6-1.r1i1p1f2.gr',\n",
       " 'CNRM-ESM2-1.r1i1p1f2.gr',\n",
       " 'CanESM5.r1i1p2f1.gn',\n",
       " 'GFDL-CM4.r1i1p1f1.gr1',\n",
       " 'GISS-E2-1-G.r1i1p1f1.gn',\n",
       " 'GISS-E2-1-G.r1i1p1f1.gn',\n",
       " 'GISS-E2-1-G.r1i1p1f1.gn',\n",
       " 'HadGEM3-GC31-LL.r1i1p1f3.gn',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR.r1i1p1f1.gr',\n",
       " 'IPSL-CM6A-LR-INCA.r1i1p1f1.gr',\n",
       " 'MIROC6.r11i1p1f1.gn',\n",
       " 'MIROC6.r11i1p1f1.gn',\n",
       " 'MPI-ESM-1-2-HAM.r1i1p1f1.gn',\n",
       " 'MRI-ESM2-0.r1i1p1f1.gn',\n",
       " 'NorESM2-LM.r1i1p1f1.gn',\n",
       " 'NorESM2-LM.r1i1p1f1.gn',\n",
       " 'NorESM2-MM.r1i1p1f1.gn',\n",
       " 'UKESM1-0-LL.r1i1p1f4.gn']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_only_one_variant_keys_list(keys_list = keys_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id_list = get_source_id_from_keys_list(dict_keys_list)\n",
    "\n",
    "find_first_index_for_given_source_id(given_source_id = 'IPSL-CM6A-LR', source_id_list = source_id_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_world_avg_field(dataset : xr.Dataset, field : str) -> xr.Dataset:\n",
    "\n",
    "    avg = dataset.spatial.average(field, axis=[\"X\", \"Y\"])[field]\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACCESS-CM2.r1i1p1f1.gn'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_00 = list(keys_dict_aprp)[0]\n",
    "\n",
    "key_00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_aprp_time_avgd = {key : dict_aprp[key].mean(\"month\")\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM HERE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize with one variable...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nremaining_fields = fields_to_be_avgd[1:]\\n\\nn_fields = len(remaining_fields)\\n\\nfor index in tqdm (range(n_fields), desc=\"Averaging all the variables...\"):\\n\\n    field = remaining_fields[index]\\n\\n    # avg on field\\n\\n    dict_aprp_avgd = {key : compute_world_avg_field(dataset = dict_aprp_avgd[key],\\n                                               field = field,\\n                                               )\\n                                               for key in keys_dict_aprp\\n                                               }\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "\n",
    "fields_to_be_avgd = [\n",
    "    'cld',\n",
    "    'sfc_alb',\n",
    "    'sfc_alb_clr',\n",
    "    'sfc_alb_oc',\n",
    "    'noncld',\n",
    "    'noncld_scat',\n",
    "    'noncld_abs',\n",
    "    'cld_amt',\n",
    "    'cld_scat',\n",
    "    'cld_abs'\n",
    "    ]\n",
    "\n",
    "# initialize\n",
    "\n",
    "print(\"Initialize with one variable...\\n\")\n",
    "\n",
    "field = \"noncld\"\n",
    "\n",
    "# avg on cld NOT ENCESSARY \n",
    "\n",
    "dict_aprp_avgd = {key : compute_world_avg_field(dataset = dict_aprp_time_avgd[key],\n",
    "                                               field = field,\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\"\"\"\n",
    "remaining_fields = fields_to_be_avgd[1:]\n",
    "\n",
    "n_fields = len(remaining_fields)\n",
    "\n",
    "for index in tqdm (range(n_fields), desc=\"Averaging all the variables...\"):\n",
    "\n",
    "    field = remaining_fields[index]\n",
    "\n",
    "    # avg on field\n",
    "\n",
    "    dict_aprp_avgd = {key : compute_world_avg_field(dataset = dict_aprp_avgd[key],\n",
    "                                               field = field,\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_aprp_avgd[key_00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate a common coarse grid to observe ensemble maps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a common coarse grid for all CMIP6 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to compare the different results, we wish to project them on a common map. This map needs to be coarser than all the natives grid in order to not generate false signals but still needs to be close enough to their original resolution. The solution that was found here is to generate the coarsest grid steps for latitude and longitude from the ensemble and generate a regular grid from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_coarse_grid = generate_the_common_coarse_grid(dict_aprp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate an extensive equivalent of the APRP datasets and regrid them onto a common grid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from int to ext function. regridding function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mieux écrire ces fonctions là c'est un peu le bordel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.get_cmip6_data.prepare_data.extract_climatologies import (\n",
    "    add_one_variable_to_dataset, # to extract the keys without the experiments\n",
    ")\n",
    "\n",
    "def intensive_field_to_extensive(dataset : xr.Dataset, areacella : xr.DataArray, intensive_field : str) -> xr.DataArray :\n",
    "    \"\"\" modify in place a field from a dataset \"\"\"\n",
    "\n",
    "    ### EXTRACT THE FIELD VALUES ###\n",
    "\n",
    "    var_array_intensive = dataset[intensive_field]\n",
    "    \n",
    "    ### INTENSIVE TO EXTENSIVE ###\n",
    "\n",
    "    var_array_extensive = var_array_intensive * areacella\n",
    "    \n",
    "    return var_array_extensive\n",
    "\n",
    "def extensive_field_to_intensive(dataset : xr.Dataset, areacella : xr.DataArray, extensive_field: str) ->  xr.DataArray:\n",
    "\n",
    "    \"\"\" modify in place a field from a dataset \"\"\"\n",
    "\n",
    "    ### EXTRACT THE FIELD VALUES ###\n",
    "\n",
    "    var_array_extensive = dataset[extensive_field]\n",
    "    \n",
    "    ### INTENSIVE TO EXTENSIVE ###\n",
    "\n",
    "    var_array_intensive = var_array_extensive / areacella\n",
    "    \n",
    "    return var_array_intensive\n",
    "\n",
    "def get_only_extensive_fields(all_fields : list[str], only_intensive_fields : list[str]) -> list[str]:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    ### REMOVE THE INTENSIVE FIELDS OF THE FULL FIELDS LIST ###\n",
    "\n",
    "    only_extensive_fields = [field for field in all_fields if field not in only_intensive_fields]\n",
    "\n",
    "    return only_extensive_fields\n",
    "\n",
    "\n",
    "def generate_dataset_with_only_the_extensive_fields(dataset : xr.Dataset, only_intensive_fields : list[str]) -> xr.Dataset:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    ### GET ALL FIELDS OF THE DATASET ###\n",
    "\n",
    "    all_fields =  list(dataset.keys())\n",
    "\n",
    "    ### GENERATE ONLY THE EXTENSIVE FIELDS LIST ###\n",
    "\n",
    "    only_extensive_fields = get_only_extensive_fields(all_fields = all_fields, only_intensive_fields = only_intensive_fields)\n",
    "\n",
    "    ### GENERATE THE DATASET ###\n",
    "\n",
    "    only_extensive_part_of_dataset = dataset[only_extensive_fields]\n",
    "\n",
    "    return only_extensive_part_of_dataset\n",
    "\n",
    "def partly_intensive_dataset_to_fully_extensive_dataset(dataset : xr.Dataset, intensive_fields_list : list[str]) -> xr.Dataset:\n",
    "    \"\"\"\"\"\"\n",
    "    ### EXTRACT AREACELLA AND INITALIAZE A NEW FULLY EXTENSIVE DATAARRAY ###\n",
    "\n",
    "    areacella_dataarray = dataset.areacella\n",
    "\n",
    "    ### FULL LIST OF FIELDS ###\n",
    "\n",
    "    all_fields = list(dataset.keys())\n",
    "\n",
    "    ### GENERATE ONLY EXTENSIVE DATASET ###\n",
    "\n",
    "    only_extensive_dataset = generate_dataset_with_only_the_extensive_fields(dataset = dataset, only_intensive_fields = intensive_fields_list)\n",
    "\n",
    "    for intensive_field_name in intensive_fields_list :\n",
    "        \n",
    "        ### generate the extensive counterpart ###\n",
    "\n",
    "        extensive_field = intensive_field_to_extensive(dataset = dataset, areacella = areacella_dataarray, intensive_field = intensive_field_name)\n",
    "\n",
    "        ### new variable name ###\n",
    "\n",
    "        extensive_field_name = intensive_field_name + \"_extensive\"\n",
    "\n",
    "        ### add it to the only_extensive_dataset ###\n",
    "\n",
    "        only_extensive_dataset = add_one_variable_to_dataset(variable_name = extensive_field_name, \n",
    "                                                             var_datarray = extensive_field, \n",
    "                                                             modify_data = True, \n",
    "                                                             dataset = only_extensive_dataset,\n",
    "                                                             )\n",
    "    return only_extensive_dataset\n",
    "\n",
    "def fully_extensive_dataset_to_partly_intensive_dataset(dataset : xr.Dataset, intensive_fields_list : list[str]) -> xr.Dataset:\n",
    "    \n",
    "    ### EXTRACT AREACELLA AND INITALIAZE A NEW FULLY EXTENSIVE DATAARRAY ###\n",
    "\n",
    "    areacella_dataarray = dataset.areacella\n",
    "\n",
    "    ### FULL LIST OF FIELDS ###\n",
    "\n",
    "    all_fields = list(dataset.keys())\n",
    "\n",
    "    ### TO BE TRANSFORMED VARIABLES ###\n",
    "\n",
    "    fields_to_bet_transformed_name_list = [field + \"_extensive\" for field in intensive_fields_list]\n",
    "\n",
    "    ### GENERATE THE NEW DATASET WITH INTENSIVE FIELDS ###\n",
    "\n",
    "    partly_intensive_dataset = generate_dataset_with_only_the_extensive_fields(dataset = dataset, only_intensive_fields = fields_to_bet_transformed_name_list)\n",
    "\n",
    "    for intensive_field_name in intensive_fields_list :\n",
    "\n",
    "        extensive_field_to_return_intensive_name = intensive_field_name + \"_extensive\"\n",
    "    \n",
    "        ### go back to intensive  ###\n",
    "\n",
    "        intensive_field = extensive_field_to_intensive(dataset = dataset, areacella = areacella_dataarray, extensive_field = extensive_field_to_return_intensive_name)\n",
    "        \n",
    "        ### add it to the only_extensive_dataset ###\n",
    "\n",
    "        partly_intensive_dataset = add_one_variable_to_dataset(variable_name = intensive_field_name, \n",
    "                                                             var_datarray = intensive_field, \n",
    "                                                             modify_data = True, \n",
    "                                                             dataset = partly_intensive_dataset,\n",
    "                                                             )\n",
    "    return partly_intensive_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_world_avg(dataset : xr.Dataset, areacella : xr.DataArray, intensive_field : str) -> xr.DataArray :\n",
    "    \"\"\" modify in place a field from a dataset \"\"\"\n",
    "\n",
    "    ### EXTRACT THE FIELD VALUES ###\n",
    "\n",
    "    var_array_intensive = dataset[intensive_field]\n",
    "    \n",
    "    ### INTENSIVE TO EXTENSIVE ###\n",
    "\n",
    "    var_array_extensive = var_array_intensive * areacella\n",
    "    \n",
    "    return var_array_extensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "\n",
    "def compute_grid_areacella(grid : xr.Dataset) -> NDArray[np.float64] :\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ### INITIALIZATION ###\n",
    "\n",
    "    ## Define the radius of the earth ##\n",
    "    \n",
    "    R_earth = 6371.0 * 10**3\n",
    "\n",
    "    ## Define the lat and lon arrays ##\n",
    "\n",
    "    latitude = grid.lat\n",
    "\n",
    "    longitude = grid.lon\n",
    "\n",
    "    ## Define the steps in radians ##\n",
    "\n",
    "    # In degreees #\n",
    "\n",
    "    dlon = np.mean(np.diff(longitude))\n",
    "\n",
    "    dlat = np.mean(np.diff(latitude))\n",
    "\n",
    "    # In rad #\n",
    "    \n",
    "    dlon_rad = np.deg2rad(dlon)\n",
    "    \n",
    "    dlat_rad = np.deg2rad(dlat)\n",
    "\n",
    "    print(dlon, dlat)\n",
    "\n",
    "    ## Define the shape of the grid ##\n",
    "\n",
    "    n_lat = grid.lat.size\n",
    "\n",
    "    n_lon = grid.lon.size\n",
    "\n",
    "    ### COMPUTATION OF AREACELLA FOR ONE MODEL ###\n",
    "\n",
    "    ## Definition ##\n",
    "    \n",
    "    areacella = np.zeros((n_lat,n_lon))\n",
    "\n",
    "    ## Cycle for ##\n",
    "    \n",
    "    for ii_lat,lat in enumerate(latitude):\n",
    "\n",
    "        for ii_lon,lon in enumerate(longitude):\n",
    "\n",
    "            areacella[ii_lat,ii_lon] = (R_earth*dlat_rad)*(R_earth*np.cos(np.deg2rad(lat))*dlon_rad)\n",
    "\n",
    "    return areacella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_00 = list(dict_aprp.keys())[0]\n",
    "\n",
    "ds = dict_aprp[key_00]\n",
    "\n",
    "intensive_fields_list = [\n",
    "    \"cld\",\n",
    "    \"sfc_alb\",\n",
    "    \"sfc_alb_clr\",\n",
    "    \"sfc_alb_oc\",\n",
    "    \"noncld\",\n",
    "    \"noncld_scat\",\n",
    "    \"noncld_abs\",\n",
    "    \"cld_amt\",\n",
    "    \"cld_scat\",\n",
    "    \"cld_abs\",\n",
    "]\n",
    "\n",
    "only_extensive_dataset = partly_intensive_dataset_to_fully_extensive_dataset(dataset = ds,\n",
    "                                                                             intensive_fields_list = intensive_fields_list,\n",
    "                                                                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally compute the regridded APRP values by dividing by the new grid points areacella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code an areacella function. code a from extensive to intensive function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If performing conservative regridding from a high/medium resolution lat/lon grid to a coarse lat/lon target, Regrid2 may provide better results as it assumes grid cells with constant latitudes and longitudes while xESMF assumes the cells are connected by Great Circles : https://xcdat.readthedocs.io/en/latest/generated/xarray.Dataset.regridder.horizontal.html\n",
    "\n",
    "\n",
    "BASE ARRAY NO VALUE AT THE POLES ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ds.regridder.horizontal(\"sfc_alb\", common_coarse_grid, tool=\"regrid2\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 4))\n",
    "\n",
    "ds.sfc_alb.isel(time=0).plot(\n",
    "    ax=axes[0], vmin=-40, vmax=40, extend=\"both\", cmap=\"RdBu_r\"\n",
    ")\n",
    "\n",
    "output.sfc_alb.isel(time=0).plot(\n",
    "    ax=axes[1], vmin=-40, vmax=40, extend=\"both\", cmap=\"RdBu_r\"\n",
    ")\n",
    "axes[1].set_title(\"Output data\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extraire une même variable, regrid toute la variable et combiner sur le nb de modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_field(dataset : xr.Dataset, field: str, output_grid : xr.Dataset):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    field_regridded = dataset.regridder.horizontal(field, output_grid, tool=\"regrid2\")\n",
    "\n",
    "    return field_regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "intensive_fields_list = [\n",
    "    \"cld\",\n",
    "    \"sfc_alb\",\n",
    "    \"sfc_alb_clr\",\n",
    "    \"sfc_alb_oc\",\n",
    "    \"noncld\",\n",
    "    \"noncld_scat\",\n",
    "    \"noncld_abs\",\n",
    "    \"cld_amt\",\n",
    "    \"cld_scat\",\n",
    "    \"cld_abs\",\n",
    "]\n",
    "\n",
    "dict_aprp_fully_extensive = {key : partly_intensive_dataset_to_fully_extensive_dataset(dataset = dict_aprp[key],\n",
    "                                                                                       intensive_fields_list = intensive_fields_list,\n",
    "                                                                                       )\n",
    "                                                                                       for key in keys_dict_aprp\n",
    "                                                                                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "\n",
    "fields_to_be_regridded = [\n",
    "    'cld_extensive',\n",
    "    'sfc_alb_extensive',\n",
    "    'sfc_alb_clr_extensive',\n",
    "    'sfc_alb_oc_extensive',\n",
    "    'noncld_extensive',\n",
    "    'noncld_scat_extensive',\n",
    "    'noncld_abs_extensive',\n",
    "    'cld_amt_extensive',\n",
    "    'cld_scat_extensive',\n",
    "    'cld_abs_extensive'\n",
    "    ]\n",
    "\n",
    "# initialize\n",
    "\n",
    "print(\"Initialize with one variable...\\n\")\n",
    "\n",
    "field = \"cld_extensive\"\n",
    "\n",
    "dict_aprp_regridded_full = {key : regrid_field(dataset = dict_aprp_fully_extensive[key],\n",
    "                                               field = field,\n",
    "                                               output_grid = common_coarse_grid\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\n",
    "remaining_fields = fields_to_be_regridded[1:]\n",
    "n_fields = len(remaining_fields)\n",
    "\n",
    "for index in tqdm (range(n_fields), desc=\"Regridding all the variables...\\n\"):\n",
    "\n",
    "    field = remaining_fields[index]\n",
    "    # generate the dictionnary of all models for one field\n",
    "\n",
    "    dict_aprp_regridded_given_field = {key : regrid_field(dataset = dict_aprp_fully_extensive[key],\n",
    "                                               field = field,\n",
    "                                               output_grid = common_coarse_grid\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\n",
    "    # add this one field to the full dictionnary\n",
    "\n",
    "    dict_aprp_regridded_full = {\n",
    "        key: add_one_variable_to_dataset(\n",
    "            variable_name=field,\n",
    "            var_datarray=dict_aprp_regridded_given_field[key][field],\n",
    "            modify_data=True,\n",
    "            dataset=dict_aprp_regridded_full[key],\n",
    "        )\n",
    "        for key in keys_dict_aprp\n",
    "    }\n",
    "\n",
    "\n",
    "output_areacella = compute_grid_areacella(common_coarse_grid)\n",
    "\n",
    "### ADD AREACELLA ###\n",
    "\n",
    "print(\"Add areacella...\\n\")\n",
    "\n",
    "for key in keys_dict_aprp :\n",
    "\n",
    "    dataset_given_exp = dict_aprp_regridded_full[key]\n",
    "\n",
    "    dataset_given_exp[\"areacella\"] = (\n",
    "            (\"lat\", \"lon\"),\n",
    "            output_areacella,\n",
    "        )\n",
    "    \n",
    "    dict_aprp_regridded_full[key] = dataset_given_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without intensive transition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "\n",
    "fields_to_be_regridded = [\n",
    "    'cld',\n",
    "    'sfc_alb',\n",
    "    'sfc_alb_clr',\n",
    "    'sfc_alb_oc',\n",
    "    'noncld',\n",
    "    'noncld_scat',\n",
    "    'noncld_abs',\n",
    "    'cld_amt',\n",
    "    'cld_scat',\n",
    "    'cld_abs'\n",
    "    ]\n",
    "\n",
    "# initialize\n",
    "\n",
    "print(\"Initialize with one variable...\\n\")\n",
    "\n",
    "field = \"cld\"\n",
    "\n",
    "dict_aprp_regridded = {key : regrid_field(dataset = dict_aprp[key],\n",
    "                                               field = field,\n",
    "                                               output_grid = common_coarse_grid\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\n",
    "remaining_fields = fields_to_be_regridded[1:]\n",
    "n_fields = len(remaining_fields)\n",
    "\n",
    "for index in tqdm (range(n_fields), desc=\"Regridding all the variables...\"):\n",
    "\n",
    "    field = remaining_fields[index]\n",
    "    # generate the dictionnary of all models for one field\n",
    "\n",
    "    dict_aprp_regridded_given_field = {key : regrid_field(dataset = dict_aprp[key],\n",
    "                                               field = field,\n",
    "                                               output_grid = common_coarse_grid\n",
    "                                               )\n",
    "                                               for key in keys_dict_aprp\n",
    "                                               }\n",
    "\n",
    "    # add this one field to the full dictionnary\n",
    "\n",
    "    dict_aprp_regridded = {\n",
    "        key: add_one_variable_to_dataset(\n",
    "            variable_name=field,\n",
    "            var_datarray=dict_aprp_regridded_given_field[key][field],\n",
    "            modify_data=True,\n",
    "            dataset=dict_aprp_regridded[key],\n",
    "        )\n",
    "        for key in keys_dict_aprp\n",
    "    }\n",
    "\n",
    "\n",
    "output_areacella = compute_grid_areacella(common_coarse_grid)\n",
    "\n",
    "### ADD AREACELLA ###\n",
    "\n",
    "print(\"Add areacella...\\n\")\n",
    "\n",
    "for key in keys_dict_aprp :\n",
    "\n",
    "    dataset_given_exp = dict_aprp_regridded[key]\n",
    "\n",
    "    dataset_given_exp[\"areacella\"] = (\n",
    "            (\"lat\", \"lon\"),\n",
    "            output_areacella,\n",
    "        )\n",
    "    \n",
    "    dict_aprp_regridded[key] = dataset_given_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict_aprp = dict_aprp.keys()\n",
    "\n",
    "intensive_fields_list = [\n",
    "    \"cld\",\n",
    "    \"sfc_alb\",\n",
    "    \"sfc_alb_clr\",\n",
    "    \"sfc_alb_oc\",\n",
    "    \"noncld\",\n",
    "    \"noncld_scat\",\n",
    "    \"noncld_abs\",\n",
    "    \"cld_amt\",\n",
    "    \"cld_scat\",\n",
    "    \"cld_abs\",\n",
    "]\n",
    "\n",
    "\n",
    "dict_aprp_regridded_full_intensive = {key : fully_extensive_dataset_to_partly_intensive_dataset(dataset = dict_aprp_regridded_full[key],\n",
    "                                                                                       intensive_fields_list = intensive_fields_list,\n",
    "                                                                                       )\n",
    "                                                                                       for key in keys_dict_aprp\n",
    "                                                                                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for the concat function to work\n",
    "\n",
    "dict_aprp_regridded = {\n",
    "    key: dict_aprp_regridded[key]\n",
    "    .groupby(dict_aprp_regridded[key].time.dt.month)\n",
    "    .mean()\n",
    "    for key in keys_dict_aprp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make the entry average\n",
    "\n",
    "ensemble_aprp_avg = xr.concat(dict_aprp_regridded.values(), \"entry\").mean(\n",
    "    dim=\"entry\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT MAP OF APRP COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ensemble_aprp_avg\n",
    "\n",
    "fields = [\n",
    "    \"cld\",\n",
    "    \"sfc_alb\",\n",
    "    \"noncld\",\n",
    "    \"noncld_scat\",\n",
    "    \"noncld_abs\",\n",
    "    \"cld_amt\",\n",
    "    \"cld_scat\",\n",
    "    \"cld_abs\",\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.suptitle(\"APRP Components\", fontsize=16, x=0, ha=\"left\")\n",
    "axes = fig.subplots(nrows=4, ncols=2, subplot_kw={\"projection\": ccrs.Robinson()})\n",
    "cnt = -1\n",
    "for row in range(4):\n",
    "    for col in range(2):\n",
    "        cnt += 1\n",
    "        var = fields[cnt]\n",
    "        avgmap = output.mean(\"month\")\n",
    "        avg = avgmap.spatial.average(var, axis=[\"X\", \"Y\"])[var].values\n",
    "        pl = avgmap[var].plot(\n",
    "            ax=axes[row, col],\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            vmin=-7.5,\n",
    "            vmax=7.5,\n",
    "            cmap=\"RdBu_r\",\n",
    "            add_colorbar=False,\n",
    "        )\n",
    "        axes[row, col].set_title(var + \" (\" + str(np.round(avg, 3)) + \")\")\n",
    "        axes[row, col].coastlines()\n",
    "\n",
    "plt.tight_layout(w_pad=2.5, h_pad=-2)\n",
    "\n",
    "fig.colorbar(\n",
    "    pl, ax=axes.ravel().tolist(), pad=0.02, shrink=0.5, aspect=15, label=\"W/m$^2$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cmip6-analysis]",
   "language": "python",
   "name": "conda-env-cmip6-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
