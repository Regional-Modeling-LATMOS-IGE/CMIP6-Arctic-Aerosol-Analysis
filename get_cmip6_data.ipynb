{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc97a7c-a97e-4464-9d20-e22becb5685b",
   "metadata": {},
   "source": [
    "# Downloading and preparing an ensemble of CMIP6 data obtained with a series of criterias thanks to intake-esgf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b1ede-2de5-487c-9602-6b1b339c6e44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Purpose of the notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44b794",
   "metadata": {},
   "source": [
    "This notebook aims at **downloading an ensemble of CMIP6 variables from python** thanks to a dictionary of **user-defined criterias**. It then **process to prepare the raw data** to make it into a **dictionary monthly climatologies** suitable for an analysis python code.\n",
    "\n",
    "This notebook introduces routines that are coded in functions in the /utilities subfolder of the repository. Namely, we are using routines that are coded in *folders_handle*, *load_raw_data*, *store_data* and *prepare_data*.\n",
    "\n",
    "The user should expect a running time of about 2 hours if the data is not present on disk.\n",
    "\n",
    "All the links of the documents were accessed on the **25/03/2025**.\n",
    "\n",
    "It uses the *intake-esgf* library : https://github.com/esgf2-us/intake-esgf?tab=readme-ov-file. A beginner guide for this library can be found there : https://intake-esgf.readthedocs.io/en/latest/beginner.html.\n",
    "\n",
    "A detailed documentation for the CMIP6 can be found here : https://wcrp-cmip.org/cmip-model-and-experiment-documentation/.\n",
    "\n",
    "Feel free to share, use and improve the following code according to the provided license on the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dc94a-da9d-4935-b2f2-980f174b2e94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model outputs searched\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3801d",
   "metadata": {},
   "source": [
    "Every search needs to be constrained by the attributes of the model outputs we are looking for. A detailed document listing these attributes can be found here : https://docs.google.com/document/d/1h0r8RZr_f3-8egBMMh7aqLwy3snpD6_MrDz1q8n5XUk/edit?tab=t.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fed764-1686-4617-b8db-4eb9124a0030",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7090b8-596e-4ed9-bab7-6ace8abf1da2",
   "metadata": {},
   "source": [
    "We use two experiments realized during the CMIP6  : **piClim-control** and **piClim-aer**. These are both atmosphere-only climate model simulations in which sea surface temperatures (SSTs) and sea icea concentrations (SICs) are fixed at model-specific preindustrial climatological values. The description of the experiments can be found here : https://wcrp-cmip.github.io/CMIP6_CVs/docs/CMIP6_experiment_id.html.\n",
    "\n",
    "> **piClim-control** : assumes aerosols' burdens set to their preindustrial levels, it is the control experiment.\n",
    "> \n",
    "> **piClim-aer** : uses present-day, present-day being 2014, aerosols burdens' levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9468df7-ef52-4b1f-a170-d8aaef381f57",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780d594-a1a1-4332-8a62-bb9aae82697f",
   "metadata": {},
   "source": [
    "The variable used are listed and explicited below according to : https://clipc-services.ceda.ac.uk/dreq/mipVars.html.\n",
    "\n",
    "> <span style=\"color:SkyBlue\">**clt**</span>  : Total cloud area fraction (%) for the whole atmospheric column\n",
    ">\n",
    "> <span style=\"color:gold\">**rsdt**</span> : Shortwave radiation ($W/m^{2}$) **incident** at the TOA\n",
    "> \n",
    "> <span style=\"color:orange\">**rsut**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at the TOA\n",
    ">\n",
    "> <span style=\"color:orangered\">**rsutcs**</span> : Shortwave radiation ($W/m^{2}$) **going out**  at TOA for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:Orchid\">**rsds**</span> : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface\n",
    "> \n",
    "> <span style=\"color:Indigo \">**rsdscs**</span>  : Shortwave **downwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    "> \n",
    "> <span style=\"color:YellowGreen\">**rsus**</span> : Shortwave **upwelling** radiation ($W/m^{2}$) at the surface\n",
    ">\n",
    "> <span style=\"color:Darkgreen\">**rsuscs**</span>: Shortwave **upwelling** radiation ($W/m^{2}$) at the surface for **clear-sky conditions**\n",
    ">\n",
    "> **areacella** : For every grid, the latitude-dependent surface associated to each grid point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389886f6-2e8d-4d99-b4cc-71ebd86e01fb",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a82e4-19a5-4bb9-aa5d-ccb2c6260107",
   "metadata": {},
   "source": [
    "The table sets how the variables are organized. We use the **Amon** table. The details about the tables can be found here : https://clipc-services.ceda.ac.uk/dreq/index/miptable.html.\n",
    "\n",
    "> **Amon** stands for a set of monthly atmospheric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb1cc5-febe-4ebf-8dca-b8700098e548",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initialisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdebe37-a1de-4df2-a44b-1115d2a8c4dd",
   "metadata": {},
   "source": [
    "### Importations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63355f-8ed1-4a49-aef4-82919ba07cf8",
   "metadata": {},
   "source": [
    "We import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dab12c-5141-4e6f-a908-aa5792b296ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ IMPORTATIONS ================ #\n",
    "\n",
    "### DOWNLADING THE ENTRIES ###\n",
    "\n",
    "import intake_esgf  # this gives us access to the ESGF catalog to make queries\n",
    "\n",
    "### DATA OBJECTS AND ASSOCIATED COMPUTATION ###\n",
    "\n",
    "import pandas as pd  # to manage the product of the search\n",
    "\n",
    "import numpy as np  # to manage the pandas arrays\n",
    "\n",
    "### HANDLE PATHS ###\n",
    "\n",
    "import os  # to get access to commands related to path setting and creation of directories\n",
    "\n",
    "### PROGRESS BAR ###\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "### HOMEMADE ROUTINES ###\n",
    "\n",
    "## Bash folder routines from python ##\n",
    "\n",
    "from utilities.get_cmip6_data.folders_handle.create import (\n",
    "    create_dir,  # function to create a cleaned downloading directory\n",
    ")\n",
    "\n",
    "## Per-entry download routine ##\n",
    "\n",
    "from utilities.get_cmip6_data.load_raw_data.load_cmip6 import (\n",
    "    update_single_entry_keys,  # makes the good key format for the full cmip6 dictionary\n",
    "    generate_single_model_search_criterias,  # generates search criterias for a single model\n",
    ")\n",
    "\n",
    "## Convert a dictionary into a netcf file and vice-versa ##\n",
    "\n",
    "from utilities.get_cmip6_data.store_data.dict_netcdf_transform import (\n",
    "    dict_to_netcdf,  # transform a dictionary of xarray datasets into a set netcdf file\n",
    ")\n",
    "\n",
    "## Generate the monthly climatologies for every entry and experiments ##\n",
    "\n",
    "from utilities.get_cmip6_data.prepare_data.extract_climatologies import (\n",
    "    create_climatology_dict,  # this function generate the dictionary made of monthly climatology xarrays\n",
    "    generate_per_model_dict_key,  # this function uses the key of the raw data dictionary to generate an unique key list per entry and experiment\n",
    "    add_one_variable_to_dataset,  # this function adds a variable to a dataset (it can create it)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474b9b5-edeb-4cc5-829c-035af5c986ff",
   "metadata": {},
   "source": [
    "### Set our search criterias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d13d48-a18d-4f4c-ae0c-32719909fdc6",
   "metadata": {},
   "source": [
    "Here the user may define its search criterias. We create a dictionary structure whose keys are the search criterias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8159458-7777-4fe5-83af-84d953ea377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ SEARCH CRITERIAS FOR OUR ANALYSIS ================ #\n",
    "\n",
    "### EXPERIMENTS ###\n",
    "\n",
    "experiment_id = [\n",
    "    \"piClim-control\",\n",
    "    \"piClim-aer\",\n",
    "]\n",
    "\n",
    "### VARIABLES ###\n",
    "\n",
    "variable_id = [\n",
    "    \"clt\",\n",
    "    \"rsdt\",\n",
    "    \"rsut\",\n",
    "    \"rsutcs\",\n",
    "    \"rsds\",\n",
    "    \"rsus\",\n",
    "    \"rsdscs\",\n",
    "    \"rsuscs\",\n",
    "]\n",
    "\n",
    "### TABLE ###\n",
    "\n",
    "table_id = \"Amon\"\n",
    "\n",
    "### DEFINE SEARCH CRITERIAS dictionary ###\n",
    "\n",
    "search = {\n",
    "    \"experiment_id\": experiment_id,\n",
    "    \"variable_id\": variable_id,\n",
    "    \"table_id\": table_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9f4bd-93d4-48be-8384-917bc9a707c2",
   "metadata": {},
   "source": [
    "### Create the folder in which the data will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11ada8",
   "metadata": {},
   "source": [
    "Here the user can chose to create the folder to store the data. It will erase a pre-existing folder if the *do_we_clear* option is set to **True**. This can be quite slow if the data folder is already holding some heavy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CHOSE IF WE CLEAR AN ALREADY EXISTING FOLDER ================ #\n",
    "\n",
    "do_we_clear = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff16fda",
   "metadata": {},
   "source": [
    "Next, the user needs to define the paths at which will be downloaded the data and saved the climatologies. These paths are the absolute paths from the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415429ca-8b8a-4fbb-9aa5-75775894c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CREATE THE FOLDER TO STORE THE DOWNLOADED DATA ================ #\n",
    "\n",
    "### DEFINE WHERE TO MOVE THE FILES AT THE END ###\n",
    "\n",
    "## Home directory ##\n",
    "\n",
    "homedir_path = os.path.expanduser(\"~\")\n",
    "\n",
    "## Parent directory ##\n",
    "\n",
    "parent_path = homedir_path + \"/certainty-data\"\n",
    "\n",
    "## Name of the created folder ##\n",
    "\n",
    "downloading_folder_name = \"CMIP6-DATA\"\n",
    "\n",
    "### CREATE THE DIRECTORY AND EMPTY IF MAKE_A_NEW_FOLDER ###\n",
    "\n",
    "downloading_path = create_dir(\n",
    "    parent_path=parent_path, name=downloading_folder_name, clear=do_we_clear\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The downloading folder {} is under the path {}.\".format(\n",
    "        downloading_folder_name, downloading_path\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c8be4-7c3d-4206-8519-af1898501d0c",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure the ESGFCatalog\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856c1ec-613e-414c-9690-086a43e26a16",
   "metadata": {},
   "source": [
    "The ESGFCatalog is initially parametrized with default values one may want to change. We will focus on three main changes :\n",
    "\n",
    "* **defining the nodes that the query will investigate** \n",
    "\n",
    "* **setting where the data will be downloaded** \n",
    "\n",
    "* **adding a path that is specific to our cluster to search for CMIP6 outputs locally** \n",
    "\n",
    "The default configuration of the catalog can be accessed through the following line of code. More details on the configuration may be found here : https://intake-esgf.readthedocs.io/en/latest/configure.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4a1db-5472-4a50-87d1-eb44f75d0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intake_esgf.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37676875-e50f-443d-98ef-50ef80c75b5a",
   "metadata": {},
   "source": [
    "There are some variables that are of interest to us :\n",
    "\n",
    "* The *solr_indices* and the *globus_indices* variables define on which nodes the query is realized. \n",
    "\n",
    "* The *local_cache* variable sets where the data will be downloaded.\n",
    "\n",
    "* The *esg_dataroot* variable sets local path to explore before downloading any data.\n",
    "\n",
    "We can see that, as said in the documentation of the intake_esgf library, the search is done by default with Globus-based indices at the holdings of OLCF (Oak Ridge Leadership Computing Facility) and ALCF (Argonne Leadership Computing Facility). We may extend the search to all the possible ESGF nodes in order to not miss any model output. Note that the solr nodes are way much slower than the globus ones.\n",
    "\n",
    "By default, the folder in which is stored the downloaded data is ~/.esgf/. This is a hidden folder in your home repository. It is more convenient, if you are working on a shared resource such as an institutional cluster or group workstation, to define it on a directory dedicated to data.\n",
    "\n",
    "Finally, if we are working on a cluster having some access to CMIP6 data, it is worth adding the path of this data in our cluster to avoid useless downloading.\n",
    "\n",
    "In the following part we will show how to modify these variables. \n",
    "\n",
    "**To decide if you want to modify them you need to set the following variables to True or define them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01235f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CONFIGURE THE ESGF CATALOG ================ #\n",
    "\n",
    "### USE ALL NODES FOR SEARCH ###\n",
    "\n",
    "all_indices = True\n",
    "\n",
    "### SET A NEW DOWNLOADING PATH ###\n",
    "\n",
    "set_new_downloading_path = True\n",
    "\n",
    "### SET A CLUSTER SPECIFIC CMIP6 PATH ###\n",
    "\n",
    "cluster_local_CMIP6_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24386ed-c4b0-43fd-bb75-2ad48ef07bca",
   "metadata": {},
   "source": [
    "### Define the nodes for the research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732995b",
   "metadata": {},
   "source": [
    "We may decide to look at all the nodes. Note that the solr nodes are way much slower than the globus ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f5807-44b0-4152-af85-c23a574394ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_esgf.conf.set(all_indices=all_indices)\n",
    "\n",
    "if all_indices:\n",
    "\n",
    "    print(\"We are looking at all the nodes.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"We are only looking at the globus nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137085e-e87b-4539-b175-ce77c775195b",
   "metadata": {},
   "source": [
    "### Set where the data will be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35ff08-7788-4199-8596-f977b9f9c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_new_downloading_path:\n",
    "\n",
    "    intake_esgf.conf.set(local_cache=downloading_path)\n",
    "\n",
    "    print(\n",
    "        \"The CMIP6 data will be downloaded at the path : {}\".format(\n",
    "            intake_esgf.conf[\"local_cache\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4aaaf-d574-4f92-af6e-47c8d0673075",
   "metadata": {},
   "source": [
    "### Add a cluster-specific CMIP6 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c98fe2-3864-4373-a6cd-ebbc2dded7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_local_CMIP6_path != \"\":\n",
    "\n",
    "    intake_esgf.conf.set(esg_dataroot=cluster_local_CMIP6_path)\n",
    "\n",
    "    print(\n",
    "        \"The CMIP6 data will be searched beforehand at the path : {}\".format(\n",
    "            intake_esgf.conf[\"esg_dataroot\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No local cluster-specific CMIP6 path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d5d09",
   "metadata": {},
   "source": [
    "### Print the new configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intake_esgf.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a19cd6-d165-428f-984f-9133f7339dc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Make the query to the ESGFCatalog\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380bc91-38b4-4748-9041-5c43fe523818",
   "metadata": {},
   "source": [
    "### Initialise the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a145bbb-ad2d-4545-a8ed-625365507741",
   "metadata": {},
   "source": [
    "The catalog variable is initially empty and will be filled given the criterias that we will impose for the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d445ed-8924-4f62-ba7c-7e4644e0d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake_esgf.ESGFCatalog()\n",
    "\n",
    "print(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d3079-ebc4-41a0-aa62-38c62de2e50c",
   "metadata": {},
   "source": [
    "### Constrain the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25edd29",
   "metadata": {},
   "source": [
    "We apply the criterias defined earlier to the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ebabc-cb5d-48de-86d2-a45172e67696",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.search(**search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc077191",
   "metadata": {},
   "source": [
    "### Convert the catalog results into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b067741",
   "metadata": {},
   "source": [
    "The resulting catalog can be converted to a pandas dataframe. This is convenient to isolate some properties of the catalog like the models' names that it has found also known as the *source_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ CONVERT TO PANDAS DATAFRAME ================ #\n",
    "\n",
    "initial_search_df = catalog.df\n",
    "\n",
    "### SHOW THE SOURCE_ID THAT APPEAR AT LEAST ONCE ###\n",
    "\n",
    "## Extract the list of the models' names ##\n",
    "\n",
    "# Retrieve the source_id column and take only one example for every duplicate #\n",
    "\n",
    "list_model_names = initial_search_df.source_id.unique()\n",
    "\n",
    "## Print the result ##\n",
    "\n",
    "print(\"The list of found models' names is : \\n{}\".format(list_model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2028de0",
   "metadata": {},
   "source": [
    "### Regroup the results by model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc6fb3",
   "metadata": {},
   "source": [
    "The number of found results is very large. They are numerous duplicates that come from the different *member_id*  and *grid_label* available for each model. \n",
    "\n",
    "The *member_id* or *variant_label* is described by 4 indices defining an ensemble member: *r* for realization, *i* for initialization, *p* for physics, and *f* for forcing. These parameters define an ensemble of experiments that correspond to the main experiment conditions for a given model. Actually, modellers may initialize their model from a different point in time, change the parametrization of a given parameter and so on.\n",
    "\n",
    "Let's regroup the results according to **(model, variant, grid)** tuples. From now on, we will refer one row of this table as an **entry**, that is to say, the whole bunch of variables and experiments associated to one tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f50635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_models = catalog.model_groups()\n",
    "\n",
    "print(grouped_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe084874",
   "metadata": {},
   "source": [
    "An interesting thing to note is the ultimate column that tells us the number of found results for a given model, variant and grid. In our case since we are looking for **8** variables for **two experiments**, the search will be deemed complete if we find **16 resulting files**.\n",
    "\n",
    "The next step is therefore to get rid of the incomplete results according to an user-defined criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62922b20",
   "metadata": {},
   "source": [
    "### Investigate for a specific model lacking variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05928fa",
   "metadata": {},
   "source": [
    "From what we have seen from the previous method, some models lack the expected number of variables. Thanks to the pandas' dataframe structure, we may investigate what are the variables missing for which experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ SEE WHAT VARIABLES ARE MISSING ================ #\n",
    "\n",
    "### LOOK AT A GIVEN MODEL ###\n",
    "\n",
    "## Define source_id and member_id ##\n",
    "\n",
    "# Source_id #\n",
    "\n",
    "looked_source_id = \"GFDL-ESM4\"\n",
    "\n",
    "# Member_id #\n",
    "\n",
    "looked_member_id = \"r1i1p1f1\"\n",
    "\n",
    "## Select the given source_id and member_id ##\n",
    "\n",
    "# Source_id #\n",
    "\n",
    "selected_source_id = initial_search_df[initial_search_df.source_id == looked_source_id]\n",
    "\n",
    "# Member_id #\n",
    "\n",
    "selected_model = selected_source_id[selected_source_id.member_id == looked_member_id]\n",
    "\n",
    "## Generate the variable_id series for the piClim-control experiment ##\n",
    "\n",
    "variable_id_for_control_exp = selected_model[\n",
    "    selected_model.experiment_id == \"piClim-control\"\n",
    "].variable_id\n",
    "\n",
    "## Generate the variable_id series for the piClim-aer experiment ##\n",
    "\n",
    "variable_id_for_aer_exp = selected_model[\n",
    "    selected_model.experiment_id == \"piClim-aer\"\n",
    "].variable_id\n",
    "\n",
    "### FIND VARIABLES THAT ARE MISSING FOR BOTH EXPERIMENTS ###\n",
    "\n",
    "## Make the criteria variable_id list as a panda series ##\n",
    "\n",
    "searched_variable_ids = pd.Series(variable_id, dtype=str)\n",
    "\n",
    "## Get the variables that are found in at least one experiment ##\n",
    "\n",
    "var_in_at_least_one = pd.Series(\n",
    "    np.union1d(variable_id_for_control_exp, variable_id_for_aer_exp), dtype=str\n",
    ")\n",
    "\n",
    "## See the missing ones compared to the variable_id list ##\n",
    "\n",
    "print(\n",
    "    \"Variables missing in both experiments of {}.{} : \\n\".format(\n",
    "        looked_source_id, looked_member_id\n",
    "    )\n",
    ")\n",
    "\n",
    "missing_variables_from_both = searched_variable_ids[\n",
    "    ~searched_variable_ids.isin(var_in_at_least_one)\n",
    "].values\n",
    "\n",
    "print(missing_variables_from_both)\n",
    "\n",
    "### GET THE VARIABLES MISSING IN ONLY ONE EXPERIMENT ###\n",
    "\n",
    "## Extract the variables found in both experiments ##\n",
    "\n",
    "var_in_both = pd.Series(\n",
    "    np.intersect1d(variable_id_for_control_exp, variable_id_for_aer_exp), dtype=str\n",
    ")\n",
    "\n",
    "## Get the variables not in common between the experiments ##\n",
    "\n",
    "# we keep the variables that are not in both experiments\n",
    "# by removing the ones that are found in both\n",
    "\n",
    "var_not_in_common = var_in_at_least_one[~var_in_at_least_one.isin(var_in_both)]\n",
    "\n",
    "## See the missing ones compared to the variable_id list ##\n",
    "\n",
    "missing_variables_not_in_common = searched_variable_ids[\n",
    "    searched_variable_ids.isin(var_not_in_common)\n",
    "].values\n",
    "\n",
    "## Find the concerned experiment if missing_variables_not_in_common is not empty ##\n",
    "\n",
    "# Check if it's empty #\n",
    "\n",
    "if missing_variables_not_in_common.size > 0:\n",
    "\n",
    "    # Get the number of variables found for each experiment #\n",
    "\n",
    "    number_variables_control = len(variable_id_for_control_exp.values)\n",
    "\n",
    "    number_variables_aer = len(variable_id_for_aer_exp.values)\n",
    "\n",
    "    # Get the least furnished experiment #\n",
    "\n",
    "    if number_variables_control < number_variables_aer:\n",
    "\n",
    "        print(\n",
    "            \"\\nOnly the control experiment of {}.{} is lacking these variables :\\n\".format(\n",
    "                looked_source_id, looked_member_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(missing_variables_not_in_common)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\n",
    "            \"\\nOnly the aerosol experiment of {}.{} is lacking these variables :\\n\".format(\n",
    "                looked_source_id, looked_member_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(missing_variables_not_in_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90eaf6a",
   "metadata": {},
   "source": [
    "### Test some filters on the incomplete results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb2ee9",
   "metadata": {},
   "source": [
    "The user may define an expected number of netcdf files for a given (model, variant,grid) tuple. In our case this is **16** as explained before. Let's impose this result by keeping only the results that match this condition. We will produce a panda series that will allow us to check if our filter is what we want.\n",
    "\n",
    "You may define a more refined filter for the results. Please look at the intake-esgf documentation for more information : https://intake-esgf.readthedocs.io/en/latest/modelgroups.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ TEST A FILTER BY GROUPING BY MODELS ================ #\n",
    "\n",
    "### SET THE EXPECTED NUMBER OF FILES ###\n",
    "\n",
    "expected_number_of_files = 16\n",
    "\n",
    "### FILTER THE INCOMPLETE RESULTS ACCORDING TO OUR CRITERIE ###\n",
    "\n",
    "filtered_results = grouped_models[grouped_models == expected_number_of_files]\n",
    "\n",
    "### PRINT THE FILTERED CATALOG ###\n",
    "\n",
    "print(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2be0a",
   "metadata": {},
   "source": [
    "The result is a pandas series that we can manipulate as so. For example if we want to retrieve all the member_id and grid_label associated to a single modell like the *IPSL-CM6A-LR*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e56605",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results[\"IPSL-CM6A-LR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d67fe",
   "metadata": {},
   "source": [
    "Good, let's have a look at the quantity of results we have left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The number of remaining (model, variant, grid) tuples is {}.\".format(\n",
    "        filtered_results.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980174f",
   "metadata": {},
   "source": [
    "### Applying the filter on the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9c50d",
   "metadata": {},
   "source": [
    "If that's satisfy us, we need to code a small function that will be **applied to the catalog**. The test will be executed on each model group that we have showed in the previous section. \n",
    "\n",
    "What's more, the pandas dataframe structure of the model groups is quite convenient as it allows us to apply more complex filters. One may want to select some specific models and variant label for example. In our case, we wish to reproduce the results from *Zelinka and al (2023)*. Therefore, we keep only the matching models and variant couples with a the provided list in the article.\n",
    "\n",
    "Firstly, we define the model.variant list of Zelinka's article.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Zelinka, M. D., Smith, C. J., Qin, Y., and Taylor, K. E.: Comparison of methods to estimate aerosol effective radiative forcings in climate models, Atmos. Chem. Phys., 23, 8879–8898, https://doi.org/10.5194/acp-23-8879-2023, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINE THE MODEL.VARIANT LIST OF ZELINKA'S ARTICLE ================ #\n",
    "\n",
    "source_id_zelinka_2023 = [\n",
    "    \"ACCESS-CM2\",\n",
    "    \"ACCESS-ESM1-5\",\n",
    "    \"BCC-ESM1\",\n",
    "    \"CESM2\",\n",
    "    \"CNRM-CM6-1\",\n",
    "    \"CNRM-ESM2-1\",\n",
    "    \"CanESM5\",\n",
    "    \"GFDL-CM4\",\n",
    "    \"GFDL-ESM4\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"HadGEM3-GC31-LL\",\n",
    "    \"IPSL-CM6A-LR-INCA\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"MIROC6\",\n",
    "    \"MIROC6\",\n",
    "    \"MPI-ESM-1-2-HAM\",\n",
    "    \"MRI-ESM2-0\",\n",
    "    \"NorESM2-LM\",\n",
    "    \"NorESM2-LM\",\n",
    "    \"NorESM2-MM\",\n",
    "    \"UKESM1-0-LL\",\n",
    "]\n",
    "\n",
    "member_id_zelinka_2023 = [\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p2f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f2\",\n",
    "    \"r1i1p3f1\",\n",
    "    \"r1i1p1f3\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r2i1p1f1\",\n",
    "    \"r3i1p1f1\",\n",
    "    \"r4i1p1f1\",\n",
    "    \"r11i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p2f1\",\n",
    "    \"r1i1p1f1\",\n",
    "    \"r1i1p1f4\",\n",
    "]\n",
    "\n",
    "zelinka_2023_model_variant_table = pd.DataFrame(\n",
    "    {\"source_id\": source_id_zelinka_2023, \"member_id\": member_id_zelinka_2023},\n",
    "    dtype=str,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8c8a4",
   "metadata": {},
   "source": [
    "Then we are able to define a filtering function that will be applied to each grouped mdoel entry individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINING THE FILTERING FUNCTION ================ #\n",
    "\n",
    "### SET THE EXPECTED NUMBER OF FILES ###\n",
    "\n",
    "expected_number_of_files = 16\n",
    "\n",
    "### DEFINE THE GLOBAL OPTIONAL PARAMETERS OF THE FILTERING FUNCTION ###\n",
    "\n",
    "filtering_by_name = True\n",
    "\n",
    "keep_only_dataframe = zelinka_2023_model_variant_table\n",
    "\n",
    "### DEFINE THE FILTERING FUNCTION ###\n",
    "\n",
    "\n",
    "def filtering_function(grouped_model_entry: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINITION ###\n",
    "\n",
    "    This function allows the intake-esgf catalog to be cleaned of the entries that are not complete.\n",
    "    In the default case it means entries that do not meet the expected number of files.\n",
    "\n",
    "    The user can also set a condition that only the model and variant couples present in a provided pandas dataframe are kept.\n",
    "    Since the nature of this function is to be an input for the intake-esgf package, we define the optional arguments outside\n",
    "    of the function.\n",
    "\n",
    "    ### INPUTS\n",
    "\n",
    "    GROUPED_MODEL_ENTRY : Pandas DataFrame | sub dataframe containing all the variables of a given source_id, member_id and grid tuple.\n",
    "\n",
    "    ### OPTIONAL ARGUMENTS (DEFINED GLOBALLY)\n",
    "\n",
    "    FILTERING_BY_NAME : BOOL | defines if we filter the entry by source_id and member_id or not\n",
    "\n",
    "    KEEP_ONLY_DATAFRAME : Pandas DataFrame | associated dataframe holding the source_id and member_id to conserve\n",
    "\n",
    "    ### OUTPUTS\n",
    "\n",
    "    BOOL | whether we keep this model group or not\n",
    "    \"\"\"\n",
    "\n",
    "    ### TEST THE NUMBER OF VARIBALES ###\n",
    "\n",
    "    if len(grouped_model_entry) == expected_number_of_files:\n",
    "\n",
    "        ### NUMBER OF VARIABLES' TEST SUCCEEDED ###\n",
    "\n",
    "        ## Do we keep only the Zelinka's article model and variant couples ? ##\n",
    "\n",
    "        # NO : We keep everything that matched the variable number's test #\n",
    "\n",
    "        if not (filtering_by_name):\n",
    "\n",
    "            return True\n",
    "\n",
    "        ### YES : KEEPING ONLY THE COUPLES PRESENT IN ZELINKA 2023 ###\n",
    "\n",
    "        else:\n",
    "\n",
    "            ## Doing the test on the grouped_model_entry's source_id and member_id ##\n",
    "\n",
    "            # Extract the grouped_model_entry data #\n",
    "\n",
    "            grouped_model_entry_source_id = grouped_model_entry.source_id.unique()[0]\n",
    "\n",
    "            grouped_model_entry_member_id = grouped_model_entry.member_id.unique()[0]\n",
    "\n",
    "            # Can we find the grouped_model_entry's source_id and member_id in one row of is_in_keep_only_dataframe ? #\n",
    "\n",
    "            is_in_keep_only_dataframe = (\n",
    "                (keep_only_dataframe[\"source_id\"] == grouped_model_entry_source_id)\n",
    "                & (keep_only_dataframe[\"member_id\"] == grouped_model_entry_member_id)\n",
    "            ).any()\n",
    "\n",
    "            # Return the result of the test #\n",
    "\n",
    "            return is_in_keep_only_dataframe\n",
    "\n",
    "    ### NUMBER OF VARIABLES' TEST FAILED ###\n",
    "\n",
    "    else:\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c222d",
   "metadata": {},
   "source": [
    "The following function allows us to apply the filtering function we have just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08053c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.remove_incomplete(filtering_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f22bd4",
   "metadata": {},
   "source": [
    "Looking at the resulting dataframe we see that we have indeed filtered the unwanted entries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7101798",
   "metadata": {},
   "source": [
    "This final catalog can be transformed into a table written for a *.md* readme file. This can be useful if one wants to show the data they use on github for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ TURN THE CATALOG INTO A TABLE FOR GITHUB ================ #\n",
    "\n",
    "### GENERATE THE TABLE ###\n",
    "\n",
    "## Create the dataframe from the catalog ##\n",
    "\n",
    "remaining_entries_dataframe = catalog.df\n",
    "\n",
    "## We extract only the needed information to describe the selected entries once ###\n",
    "\n",
    "remaining_grouped_models_dataframe = remaining_entries_dataframe[\n",
    "    [\"source_id\", \"member_id\", \"grid_label\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "## We sort the models' names by alphabetical order ##\n",
    "\n",
    "remaining_entries_dataframe_sorted = remaining_grouped_models_dataframe.sort_values(\n",
    "    [\"source_id\", \"member_id\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "### PRINTING IT FOR COPY ###\n",
    "\n",
    "print(remaining_entries_dataframe_sorted.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7907795",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Downloading the files and load a dictionary in memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e5884",
   "metadata": {},
   "source": [
    "The intake-esgf library proposes to store the found results in memory under the form of a dictionary holding xarray datasets for every single netcdf file found. This process also saves the netcdf files at the previously defined *local_cache path*. This part is less general than the previous sections as we have developed more complex routines that what the intake-esgf package provides. Still, if your request is pretty straightforward it may be relevant to first look at the intake-esgf documentation : https://intake-esgf.readthedocs.io/en/latest/quickstart.html.\n",
    "\n",
    "We have indeed developed a specific downloading routine. This is because there is an issue that may be encountered if, as in this example, the user tried to download a lot of different entries from the solr index nodes. Indeed, the downloading of some variables of the data present on these nodes will fail. A solution that has been found in this case is to make a request for a **(source_id, member_id, grid_label)** tuple at once. That is to say, **download each entry independently and then regroup them under the same dictionary**.\n",
    "\n",
    "In addition, by default, the package is looking for the **areacella** variable automatically. Areacella is the variable associated to the model's grid. It tells us the surface associated to each grid point and allows us to easily do spatial averages accross grid points. So, intake-esgf does look for areacella but it does it **rather slowly** as it first looks for an areacella variable for our full search facets, which may lead to no results, and then expands the search. Moreover, it does so for for every single variable of a given model which is redundant. In our analysis, we would rather load the dictionary with *add_measures* set to **False** and then download (and load) the areacella netcdf files apart with a homemade routine. An example of this process is given afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371e2d7",
   "metadata": {},
   "source": [
    "### Download of a lot of entries from the solr nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ab6b0",
   "metadata": {},
   "source": [
    "We prepare the download of each individual set of variables for a given model. We use for that two functions that can be found in the *load_cmip6.py* submodule in the same folder as this notebook.\n",
    "\n",
    "- The *generate_single_model_search_criterias* function which generates the *search_criterias* used to download a single entry. It is done by the needed tuple of (source_id, member_id, grid_label) criterias to the global *search* dictionary.\n",
    "\n",
    "- The *update_single_entry_keys* function which updates, once we downloaded it, the keys of the one entry dictionary to allow to concatenate all dictionaries together.\n",
    "\n",
    "With these two functions, we can generate a dictionnary holding all entries even though we downloaded them separatly.\n",
    "\n",
    "Before using them, we need to extract some information out the filtered catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ EXTRACT THE FILTERED CATALOG'S INFORMATION ================ #\n",
    "\n",
    "### GET THE INFORMATIONS THAT THE CATALOG EXTRACTED ###\n",
    "\n",
    "## Generate the full dataframe of the files found by the search ##\n",
    "\n",
    "selected_entries_full_dataframe = catalog.df\n",
    "\n",
    "## Save the grouped model pandas series for the areacella part ##\n",
    "\n",
    "series_grouped_models = catalog.model_groups()\n",
    "\n",
    "### GET ONLY THE NEEDED INFORMATION ###\n",
    "\n",
    "## We extract the (source_id, member_id, grid_label) tuples from the full dataframe ##\n",
    "\n",
    "# we remove the duplicates to only keep one row per tuple\n",
    "\n",
    "grouped_models_dataframe = (\n",
    "    selected_entries_full_dataframe[[\"source_id\", \"member_id\", \"grid_label\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b6725",
   "metadata": {},
   "source": [
    "Once we have all the needed variables, the following routine builds the full dictionary by downloading every single entry independently. This is what does the function *loading_cmip6* from the *load_cmip6* submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DOWNLOAD EVERY SINGLE OUTPUT ================ #\n",
    "\n",
    "### DOWNLOAD EVERY SINGLE ENTRY AND COMBINE THEM INTO A DICTIONARY ###\n",
    "\n",
    "## Initialize the full dictionary ##\n",
    "\n",
    "full_cmip6_dict = {}\n",
    "\n",
    "## Downloading all the models one entry at a time ##\n",
    "\n",
    "print(\"Downloading and/or loading the data one entry at a time...\\n\")\n",
    "\n",
    "for index in grouped_models_dataframe.index:\n",
    "\n",
    "    ## Reset the catalog ##\n",
    "\n",
    "    catalog = intake_esgf.ESGFCatalog()\n",
    "\n",
    "    ## Generate the associated search criterias ##\n",
    "\n",
    "    search_criterias_given_row, single_model_name = (\n",
    "        generate_single_model_search_criterias(\n",
    "            search_facets=search,\n",
    "            grouped_models_dataframe=grouped_models_dataframe,\n",
    "            index=index,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## Generate the single model's output name ##\n",
    "\n",
    "    print(\"\\nDownloading {} ...\\n\".format(single_model_name))\n",
    "\n",
    "    ## Apply the search criterias ##\n",
    "\n",
    "    catalog.search(\n",
    "        **search_criterias_given_row,\n",
    "    )\n",
    "\n",
    "    ## Downloading the output... ##\n",
    "\n",
    "    single_model_dictionary = catalog.to_dataset_dict(\n",
    "        add_measures=False,\n",
    "        ignore_facets=[\n",
    "            \"project\",\n",
    "            \"mip_era\",\n",
    "            \"activtity_drs\",\n",
    "            \"institution_id, table_id\",\n",
    "            \"grid_label\",\n",
    "            \"version\",\n",
    "        ],\n",
    "        quiet=True,\n",
    "    )\n",
    "\n",
    "    ## Updating its keys ##\n",
    "\n",
    "    single_model_dictionary = update_single_entry_keys(\n",
    "        single_model_dictionary, single_model_name\n",
    "    )\n",
    "\n",
    "    ## Updating the full dictionary ##\n",
    "\n",
    "    full_cmip6_dict = full_cmip6_dict | single_model_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ebc1e",
   "metadata": {},
   "source": [
    "### Homemade routine to retrieve the areacella of a given entry only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1398b16",
   "metadata": {},
   "source": [
    "As we have said in the introduction of this section, the main default of the intake-esgf package when it comes to look for measures is that it does so for every single variable. And this is independent of the fact that our 8 variables fore one experiment would share the same areacella grid. What's more, it always start by looking for our full set of facets which is irrevelant. \n",
    "\n",
    "Indeed, the only three parameters that define areacella are the **source_id**, **member_id** and obviously the **grid_label**. Conveniently enough, we can group our search results according to these three parameters with the *.model_groups* method. In the end, it may be that our experiment won't hold the areacella variable but it does not really matter, we just need to find one. This is the spirit of this routine made to still get areacella but significantly faster. It is worth noting that this method is not useful if you need to import a small number of models and variables as you won't really see the time difference. \n",
    "\n",
    "Let's be sure that we look at all the nodes for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e379d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5877905",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_esgf.conf.set(all_indices=all_indices)\n",
    "\n",
    "if all_indices:\n",
    "\n",
    "    print(\"We are looking at all the nodes.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"We are only looking at the globus nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a808a",
   "metadata": {},
   "source": [
    "Then we apply the following routine to produce the areacella dictionary. This is described in the function *get_areacella_apart* from the *load_cmip6* submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff61345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ HOMEMADE ROUTINE TO GET AREACELLA GRIDS FASTER ================ #\n",
    "\n",
    "### INITIALISATION  ###\n",
    "\n",
    "## Initialise the full dictionary ##\n",
    "\n",
    "dict_areacella = {}\n",
    "\n",
    "## Number of rows ##\n",
    "\n",
    "n_rows = series_grouped_models.size\n",
    "\n",
    "### GO THROUGH EVERY ROW OF THE PANDA SERIES ###\n",
    "\n",
    "for ii in range(n_rows):\n",
    "\n",
    "    ## Get the SOURCE_ID | MEMBER_ID | GRID_LABEL of the row ##\n",
    "\n",
    "    # Retrieve the row ##\n",
    "\n",
    "    row_ii = series_grouped_models.index[ii]\n",
    "\n",
    "    # Extract the labels #\n",
    "\n",
    "    source_id, member_id, grid_label = row_ii\n",
    "\n",
    "    # Build the key for this dictionary entry ##\n",
    "\n",
    "    full_key = source_id + \".\" + member_id + \".\" + grid_label\n",
    "\n",
    "    ## Special case for the IPSL-CM6A-LR-INCA model ##\n",
    "\n",
    "    if source_id == \"IPSL-CM6A-LR-INCA\":\n",
    "\n",
    "        source_id = \"IPSL-CM6A-LR\"\n",
    "\n",
    "    ## Do the full search ##\n",
    "\n",
    "    areacella_search_full = catalog.search(\n",
    "        source_id=source_id,\n",
    "        grid_label=grid_label,\n",
    "        variable_id=\"areacella\",\n",
    "        quiet=True,\n",
    "    ).df  # silence the progress bar\n",
    "\n",
    "    ## Extract the first experiment id that gives an areacella entry ##\n",
    "\n",
    "    only_first_exp_id = areacella_search_full.experiment_id.values[0]\n",
    "\n",
    "    ## Extract the first member id that gives an areacella entry ##\n",
    "\n",
    "    only_first_member_id = areacella_search_full.member_id.values[0]\n",
    "\n",
    "    ## Get the areacella for the given row ##\n",
    "\n",
    "    # Search and download it #\n",
    "\n",
    "    print(\"\\nDownloading areacella for {} ...\\n\".format(full_key))\n",
    "\n",
    "    areacella_ii = catalog.search(\n",
    "        source_id=source_id,\n",
    "        grid_label=grid_label,\n",
    "        variable_id=\"areacella\",\n",
    "        experiment_id=only_first_exp_id,\n",
    "        member_id=only_first_member_id,\n",
    "        quiet=True,\n",
    "    ).to_dataset_dict(\n",
    "        add_measures=False, quiet=True\n",
    "    )  # silence the progress bar\n",
    "\n",
    "    # Store it in dictionary #\n",
    "\n",
    "    dict_areacella[full_key] = areacella_ii[\"areacella\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925f40d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Combine the extracted data into full xarray datasets with monthly climatologies and saves them on disk\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f6050",
   "metadata": {},
   "source": [
    "The next step is to treat this data to make it usable for our analysis, in this case we wish to have monthly climatologies for every variable. We start from a bunch of netcdf files on disk associated to each entry we wished to have. \n",
    "\n",
    "However, if we wanted to retrieve this raw data in another script, we would have to call again this whole routine with intake-esgf that would, this time, find the files locally. What's more, each variable is separated under a dictionary key. This is not convenient for the generation of the climatologies as we would prefer full xarray datasets that are way more flexible.\n",
    "\n",
    "A solution that has been found here is to regroup every variable for a given experiment and a given entry under the **same xarray dataset**, generate the **monthly climatologies of every variable** and then save the datasets as **netcdf files**. The **paths** of these netcdf files are saved in a **pandas data series** that can be loaded at the beginning of a script. As a result, we can reload a simpler dictionary holding pre-treated datasets that are easier objects to manipulate without calling the ESGF catalog. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a11e7",
   "metadata": {},
   "source": [
    "### Generate monthly climatologies for every experiment and entry "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e98db4",
   "metadata": {},
   "source": [
    "First and foremost, this part aims at generating a monthly climatology for every variable of every experiment and so for every model. Then, we regroup these climatologies into two xarray datasets, one per experiment, per entry. What's more, we add the areacella variable that is for now in another dictionary as a variable of the xarray datasets. This will allow for easy weighted spatial averages.\n",
    "\n",
    "We start by defining the path where we will save the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6830f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ DEFINE THE PATHS WHERE TO SAVE THE TREATED DATA ================ #\n",
    "\n",
    "### SAVE PATH OF THE TREATED DATA ###\n",
    "\n",
    "parent_path_save_clim = (\n",
    "    homedir_path + \"/certainty-data/\" + downloading_folder_name + \"/climatologies\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bceaeb",
   "metadata": {},
   "source": [
    "We go through every variable for every entry and experiment. We thus generate monthly climatology datasets holding all the variables and areacella. These datasets are then saved into a new dictionary structure. This is what does the *create_climatology_dict* function from the *extract_climatologies* submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ GENERATE THE XARRAYS DATASETS FOR EVERY ENTRY AND EXPERIMENT ================ #\n",
    "\n",
    "### INITIALISATION ###\n",
    "\n",
    "## Create the dictionnary ##\n",
    "\n",
    "full_cmip6_dict_clim = {}\n",
    "\n",
    "## Generate the general key associated to each model.variant and experiment ##\n",
    "\n",
    "keys_without_variable_unique = generate_per_model_dict_key(full_cmip6_dict)\n",
    "\n",
    "## Define the number of unique entry and experiments couples ##\n",
    "\n",
    "n_entry_and_exp = len(keys_without_variable_unique)\n",
    "\n",
    "### GO THROUGH EACH MODEL.VARIANT.GRID AND EXPERIMENT ###\n",
    "\n",
    "## Define a progress bar while we go through the unique entry keys ##\n",
    "\n",
    "for index in tqdm(\n",
    "    range(n_entry_and_exp), desc=\"Generating the climatologies' dictionnary...\"\n",
    "):\n",
    "\n",
    "    ## Retrieve the key ##\n",
    "\n",
    "    key = keys_without_variable_unique[index]\n",
    "\n",
    "    ## Initialize the dataset with the first variable ##\n",
    "\n",
    "    # Define the variable #\n",
    "\n",
    "    var = variable_id[0]\n",
    "\n",
    "    # Define that the dataset does not exist yet #\n",
    "\n",
    "    modify_data = False\n",
    "\n",
    "    # Copy the key without variable #\n",
    "\n",
    "    key_with_var = key\n",
    "\n",
    "    # Add the variable name #\n",
    "\n",
    "    key_with_var[-1] = var\n",
    "\n",
    "    # Generate the key by joining the str list with \".\" #\n",
    "\n",
    "    key_with_var_full = \".\".join(key_with_var)\n",
    "\n",
    "    # Retrieve the variable data array #\n",
    "\n",
    "    var_datarray = full_cmip6_dict[key_with_var_full]\n",
    "\n",
    "    # Generate or update the dataset for the given model.variant and experiment #\n",
    "\n",
    "    dataset_given_exp = add_one_variable_to_dataset(\n",
    "        variable_name=var,\n",
    "        var_datarray=var_datarray,\n",
    "        modify_data=modify_data,\n",
    "        do_clim=True,\n",
    "    )\n",
    "\n",
    "    # Set that now the dataset already exists #\n",
    "\n",
    "    modify_data = True\n",
    "\n",
    "    ## Go through the rest of the variables ##\n",
    "\n",
    "    for var in variable_id[1:]:\n",
    "\n",
    "        # Copy the key without variable #\n",
    "\n",
    "        key_with_var = key\n",
    "\n",
    "        # Add the variable name #\n",
    "\n",
    "        key_with_var[-1] = var\n",
    "\n",
    "        # Generate the key by joining the str list with \".\" #\n",
    "\n",
    "        key_with_var_full = \".\".join(key_with_var)\n",
    "\n",
    "        # Retrieve the variable data array #\n",
    "\n",
    "        var_datarray = full_cmip6_dict[key_with_var_full]\n",
    "\n",
    "        # Update the dataset with the climatology of this variable #\n",
    "\n",
    "        add_one_variable_to_dataset(\n",
    "            variable_name=var,\n",
    "            var_datarray=var_datarray,\n",
    "            modify_data=modify_data,\n",
    "            dataset=dataset_given_exp,\n",
    "            do_clim=True,\n",
    "        )\n",
    "\n",
    "    ## Generate the key for full_cmip6_dict_clim ##\n",
    "\n",
    "    # Retrieving the key information #\n",
    "\n",
    "    # key =  [source_id, member_id, grid, experiment_id, '*']\n",
    "\n",
    "    source_id = key[0]\n",
    "\n",
    "    member_id = key[1]\n",
    "\n",
    "    grid_label = key[2]\n",
    "\n",
    "    experiment_id = key[3]\n",
    "\n",
    "    # Create the new key #\n",
    "\n",
    "    new_simpler_key_given_exp = \".\".join(\n",
    "        [source_id, member_id, grid_label, experiment_id]\n",
    "    )\n",
    "\n",
    "    ## Use the gathered information to get the areacella entry of the given model.variant and experiment ##\n",
    "\n",
    "    # Build the areacella key #\n",
    "\n",
    "    key_areacella = \".\".join([source_id, member_id, grid_label])\n",
    "\n",
    "    # Retrieve the given areacella #\n",
    "\n",
    "    areacella_datarray = dict_areacella[key_areacella]\n",
    "\n",
    "    # Update the dataset of the given model.variant and experiment with the associated areacella #\n",
    "\n",
    "    dataset_given_exp[\"areacella\"] = (\n",
    "        (\"lat\", \"lon\"),\n",
    "        areacella_datarray[\"areacella\"].values,\n",
    "    )\n",
    "\n",
    "    ## Add the dataset to the output dictionnary ##\n",
    "\n",
    "    full_cmip6_dict_clim[new_simpler_key_given_exp] = dataset_given_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3722c",
   "metadata": {},
   "source": [
    "We save the structure by turning every xarray dataset into netcdf files and also by generating a *key_paths_table.pkl* file. It is a way for us to save a pandas' data series, *key_paths_table*, that **associates every netcdf file's path with its key in the dictionary**. This routine is coded into the *dict_to_netcdf* function of the *store_data* submodule.\n",
    "\n",
    "Therefore, in an another script, we will able to rebuild the dictionary from scratch by using this table and the function *netcdf_to_dict* from the *store_data* submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ SAVE THE GENERATED DICTIONNARY ================ #\n",
    "\n",
    "### INITIALISATION ###\n",
    "\n",
    "## Get the list of the keys of the dictionnary #\n",
    "\n",
    "# Extract the list #\n",
    "\n",
    "list_keys = list(full_cmip6_dict_clim.keys())\n",
    "\n",
    "# Get the number of keys #\n",
    "\n",
    "n_keys = len(list_keys)\n",
    "\n",
    "## Generate the array of the paths ##\n",
    "\n",
    "paths = np.empty(n_keys, dtype=object)  # dtype = object otherwise it truncates the str\n",
    "\n",
    "### GO THROUGH THE ENTRIES ###\n",
    "\n",
    "for ii, key in enumerate(list_keys):\n",
    "\n",
    "    ## Generate a filename with the key ##\n",
    "\n",
    "    # Split the key into a list of keywords #\n",
    "\n",
    "    splitted_key = key.split(\".\")\n",
    "\n",
    "    # Connect them with a \"_\" to make a filename that is not broken #\n",
    "\n",
    "    full_name = \"_\".join(splitted_key)\n",
    "\n",
    "    # Define the filename #\n",
    "\n",
    "    filename = full_name + \".nc\"\n",
    "\n",
    "    ## Create the directory associated to the entry and keep its path ##\n",
    "\n",
    "    saving_path_given_entry = create_dir(\n",
    "        parent_path=parent_path_save_clim, name=full_name, clear=do_we_clear\n",
    "    )\n",
    "\n",
    "    ## Generate the full path with the filename ##\n",
    "\n",
    "    path_to_nc = saving_path_given_entry + \"/\" + filename\n",
    "\n",
    "    ## Save the entry's dataset ##\n",
    "\n",
    "    # Save it #\n",
    "\n",
    "    full_cmip6_dict_clim[key].to_netcdf(path=path_to_nc)\n",
    "\n",
    "    # Conserve the path at which we saved it in the array #\n",
    "\n",
    "    paths[ii] = path_to_nc\n",
    "\n",
    "### GENERATE THE PANDAS DATAFRAME ASSOCIATING KEYS WITH PATHS ###\n",
    "\n",
    "## Create the pandas dataframe from a dictionnary ##\n",
    "\n",
    "# Define the table key vs path #\n",
    "\n",
    "key_paths_dict = {\"key\": list_keys, \"path\": paths}\n",
    "\n",
    "# Define the dataframe #\n",
    "\n",
    "key_paths_table = pd.DataFrame(key_paths_dict)\n",
    "\n",
    "## Save the pandas dataframe ##\n",
    "\n",
    "# Create the table folder to hold it #\n",
    "\n",
    "saving_path_table = create_dir(\n",
    "    parent_path=parent_path_save_clim, name=\"table\", clear=do_we_clear\n",
    ")\n",
    "\n",
    "# Save it #\n",
    "\n",
    "key_paths_table.to_pickle(saving_path_table + \"/key_paths_table.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cmip6-analysis]",
   "language": "python",
   "name": "conda-env-cmip6-analysis-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
